{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"widgets":{"application/vnd.jupyter.widget-state+json":{"073ff65d11eb47c493b2ea639226b0e8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e2219883740448acb5a2156285cf8c91","placeholder":"â€‹","style":"IPY_MODEL_814e0a537755446f873ca1496fcbd530","value":"Train:   1%"}},"1911f6c9e3c94e2f8dab0d5bdaf09b1d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1e20bf164d564bb8b3cdb8d46cf722e6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"20f5274c56e346269144d86d18770793":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e20bf164d564bb8b3cdb8d46cf722e6","max":2620,"min":0,"orientation":"horizontal","style":"IPY_MODEL_506f6192973d426b8d9c66ea57aa9549","value":2620}},"3405ccac4c274c6f9a481164255bc433":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3fe3a033a7a8427a8ea38d85c15cc2d6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"506f6192973d426b8d9c66ea57aa9549":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"61f40d13cb114f5190cf81c68147f4da":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6510c6df7ed2402a9da0e5072627bb5d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6a8ba71260244df19c532e75912db8ca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"703fa46d6e8e49d786a819526f738489":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e47984c7bdd14b33a0800eb6f0ed4b9e","IPY_MODEL_20f5274c56e346269144d86d18770793","IPY_MODEL_8097df8147ee4d8a9090cd4037ae2248"],"layout":"IPY_MODEL_61f40d13cb114f5190cf81c68147f4da"}},"8097df8147ee4d8a9090cd4037ae2248":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3405ccac4c274c6f9a481164255bc433","placeholder":"â€‹","style":"IPY_MODEL_3fe3a033a7a8427a8ea38d85c15cc2d6","value":" 2620/2620 [00:03&lt;00:00, 838.04it/s]"}},"814e0a537755446f873ca1496fcbd530":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a0b6b738542c41699873e13e379dc501":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_b6708535817a43d6bf968e06c5f96108","max":1084,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1911f6c9e3c94e2f8dab0d5bdaf09b1d","value":9}},"aa000c92712f4b1a83f81c3c68fdfbee":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e09f23a2682c4916a6f48606f520258e","placeholder":"â€‹","style":"IPY_MODEL_ca58d9daa4ce43d0bcc1d943b5b965a1","value":" 9/1084 [00:14&lt;28:02,  1.56s/it, loss=3.0020, lr=0.0002, perplexity=20.2650, tf_rate=1.00]"}},"aec96d10e7014b809e757efe6a6e652f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_073ff65d11eb47c493b2ea639226b0e8","IPY_MODEL_a0b6b738542c41699873e13e379dc501","IPY_MODEL_aa000c92712f4b1a83f81c3c68fdfbee"],"layout":"IPY_MODEL_ccf14578a0184f20b58d3a8255754698"}},"b6708535817a43d6bf968e06c5f96108":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca58d9daa4ce43d0bcc1d943b5b965a1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ccf14578a0184f20b58d3a8255754698":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"e09f23a2682c4916a6f48606f520258e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e2219883740448acb5a2156285cf8c91":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e47984c7bdd14b33a0800eb6f0ed4b9e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6510c6df7ed2402a9da0e5072627bb5d","placeholder":"â€‹","style":"IPY_MODEL_6a8ba71260244df19c532e75912db8ca","value":"100%"}}}},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":62741,"databundleVersionId":6820148,"sourceType":"competition"}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# HW4P2: Attention-based Speech Recognition\n\n<img src=\"https://cdn.shopify.com/s/files/1/0272/2080/3722/products/SmileBumperSticker_5400x.jpg\" alt=\"A cute cat\" width=\"600\">\n\n\nWelcome to the final assignment in 11785. In this HW, you will work on building a speech recognition system with <i>attention</i>. <br> <br>\n\n<center>\n<img src=\"https://popmn.org/wp-content/uploads/2020/03/pay-attention.jpg\" alt=\"A cute cat\" height=\"100\">\n</center>\n\nHW Writeup: [TODO] <br>\nKaggle Competition Link: https://www.kaggle.com/competitions/attention-based-speech-recognition <br>\nKaggle Dataset Link: https://www.kaggle.com/competitions/attention-based-speech-recognition/data\n<br>\nLAS Paper: https://arxiv.org/pdf/1508.01211.pdf <br>\nAttention is all you need:https://arxiv.org/pdf/1706.03762.pdf","metadata":{"id":"8XpNMS7Vk6Df"}},{"cell_type":"markdown","source":"# Read this section importantly!","metadata":{"id":"vwIdDTTmmZVe"}},{"cell_type":"markdown","source":"1. By now, we believe that you are already a great deep learning practitioner, Congratulations. ðŸŽ‰\n\n2. You are allowed to use code from your previous homeworks for this homework. We will only provide, aspects that are necessary and new with this homework.\n\n3. There are a lot of resources provided in this notebook, that will help you check if you are running your implementations correctly.","metadata":{"id":"y9qsVrRemgh7"}},{"cell_type":"code","source":"!nvidia-smi","metadata":{"id":"8UK7J-dp5iN5","outputId":"3ebdf937-1517-43f7-e217-92f47aa9152d","execution":{"iopub.status.busy":"2023-11-29T12:01:03.451614Z","iopub.execute_input":"2023-11-29T12:01:03.452376Z","iopub.status.idle":"2023-11-29T12:01:04.425278Z","shell.execute_reply.started":"2023-11-29T12:01:03.452343Z","shell.execute_reply":"2023-11-29T12:01:04.424160Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Install some required libraries\n# Feel free to add more if you want\n!pip install -q python-levenshtein torchsummaryX wandb kaggle pytorch-nlp","metadata":{"id":"nYgaLmgy5iqR","outputId":"7db1a2f0-b17f-4c27-a1dc-bb8959932281","execution":{"iopub.status.busy":"2023-11-29T12:01:38.901158Z","iopub.execute_input":"2023-11-29T12:01:38.901577Z","iopub.status.idle":"2023-11-29T12:01:50.231090Z","shell.execute_reply.started":"2023-11-29T12:01:38.901538Z","shell.execute_reply":"2023-11-29T12:01:50.229962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torchaudio\nfrom torch import nn, Tensor\n# import torchsummary\n\nimport numpy as np\nimport os\n\nimport gc\nimport time\n\nimport pandas as pd\nfrom tqdm.notebook import tqdm as blue_tqdm\nimport matplotlib.pyplot as plt\nimport seaborn\nimport json\n\nimport math\nfrom typing import Optional, List\n\n\n#imports for decoding and distance calculation\ntry:\n    import wandb\n    import torchsummaryX\n    import Levenshtein\nexcept:\n    print(\"Didnt install some/all imports\")\n\nfrom torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(\"Device: \", DEVICE)","metadata":{"id":"0mkii-6Dsjr8","outputId":"5b1539ce-b4a3-4515-813d-9b288c8f79d8","execution":{"iopub.status.busy":"2023-11-29T12:01:58.121716Z","iopub.execute_input":"2023-11-29T12:01:58.122133Z","iopub.status.idle":"2023-11-29T12:01:58.130834Z","shell.execute_reply.started":"2023-11-29T12:01:58.122098Z","shell.execute_reply":"2023-11-29T12:01:58.130009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Config","metadata":{"id":"AIOBPQjzrx5n"}},{"cell_type":"code","source":"config = dict (\n    train_dataset       = 'train-clean-360', # train-clean-100, train-clean-360, train-clean-460\n    batch_size          = 96,\n    epochs              = 100,\n    learning_rate       = 2e-4,\n    weight_decay        = 5e-3,\n    cepstral_norm       = True,\n)","metadata":{"id":"EPchlig7rxia","execution":{"iopub.status.busy":"2023-11-29T12:02:06.091756Z","iopub.execute_input":"2023-11-29T12:02:06.092651Z","iopub.status.idle":"2023-11-29T12:02:06.097165Z","shell.execute_reply.started":"2023-11-29T12:02:06.092614Z","shell.execute_reply":"2023-11-29T12:02:06.096285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Kaggle Dataset Download","metadata":{"id":"-njBvl2Opd6I"}},{"cell_type":"code","source":"!pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\n!mkdir /root/.kaggle\n\nwith open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n    f.write('{\"username\":\"danielkwizera\",\"key\":\"858f28b48352b29cb72fa9f3815c1b5d\"}') # Put your kaggle username & key here\n\n!chmod 600 /root/.kaggle/kaggle.json","metadata":{"id":"PTyWR2sIp0Ns","outputId":"14fc963b-953e-4958-bf94-589da3a25f34","execution":{"iopub.status.busy":"2023-11-29T12:02:18.017667Z","iopub.execute_input":"2023-11-29T12:02:18.018053Z","iopub.status.idle":"2023-11-29T12:02:22.037487Z","shell.execute_reply.started":"2023-11-29T12:02:18.018023Z","shell.execute_reply":"2023-11-29T12:02:22.036200Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # # to download the dataset\n# !kaggle competitions download -c attention-based-speech-recognition\n\n# # # to unzip data quickly and quietly\n# !unzip -q attention-based-speech-recognition.zip -d ./data","metadata":{"id":"F581gjfnqE2C","outputId":"6127f379-46bd-44ab-a87f-28b36618d111","execution":{"iopub.status.busy":"2023-11-29T11:45:44.701508Z","iopub.execute_input":"2023-11-29T11:45:44.702468Z","iopub.status.idle":"2023-11-29T11:51:12.412597Z","shell.execute_reply.started":"2023-11-29T11:45:44.702430Z","shell.execute_reply":"2023-11-29T11:51:12.411374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Character-based LibriSpeech (HW4P2)\n\nIn terms of the dataset, the dataset structure for HW3P2 and HW4P2 dataset are very similar. Can you spot out the differences? What all will be required??\n\nHints:\n\n- Check how big is the dataset (do you require memory efficient loading techniques??)\n- How do we load mfccs? Do we need to normalise them?\n- Does the data have \\<SOS> and \\<EOS> tokens in each sequences? Do we remove them or do we not remove them? (Read writeup)\n- Would we want a collating function? Ask yourself: Why did we need a collate function last time?\n- Observe the VOCAB, is the dataset same as HW3P2?\n- Should you add augmentations, if yes which augmentations? When should you add augmentations? (Check bootcamp for answer)\n","metadata":{"id":"zUJyBBwIqQs6"}},{"cell_type":"code","source":"VOCAB = [\n    '<pad>', '<sos>', '<eos>',\n    'A',   'B',    'C',    'D',\n    'E',   'F',    'G',    'H',\n    'I',   'J',    'K',    'L',\n    'M',   'N',    'O',    'P',\n    'Q',   'R',    'S',    'T',\n    'U',   'V',    'W',    'X',\n    'Y',   'Z',    \"'\",    ' ',\n]\n\nVOCAB_MAP = {VOCAB[i]:i for i in range(0, len(VOCAB))}\n\nPAD_TOKEN = VOCAB_MAP[\"<pad>\"]\nSOS_TOKEN = VOCAB_MAP[\"<sos>\"]\nEOS_TOKEN = VOCAB_MAP[\"<eos>\"]\n\nprint(f\"Length of vocab : {len(VOCAB)}\")\nprint(f\"Vocab           : {VOCAB}\")\nprint(f\"PAD_TOKEN       : {PAD_TOKEN}\")\nprint(f\"SOS_TOKEN       : {SOS_TOKEN}\")\nprint(f\"EOS_TOKEN       : {EOS_TOKEN}\")","metadata":{"id":"MBMLGYX-kZcd","outputId":"7330ab8d-127b-486f-d33c-a59d4c0c22eb","execution":{"iopub.status.busy":"2023-11-29T12:02:58.637967Z","iopub.execute_input":"2023-11-29T12:02:58.638921Z","iopub.status.idle":"2023-11-29T12:02:58.646713Z","shell.execute_reply.started":"2023-11-29T12:02:58.638884Z","shell.execute_reply":"2023-11-29T12:02:58.645693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(VOCAB_MAP)","metadata":{"id":"9ERlUvyGCMuz","outputId":"2771c2a3-4f9c-448d-dcaa-6e58b9de06ea","execution":{"iopub.status.busy":"2023-11-29T12:03:01.793184Z","iopub.execute_input":"2023-11-29T12:03:01.793628Z","iopub.status.idle":"2023-11-29T12:03:01.801045Z","shell.execute_reply.started":"2023-11-29T12:03:01.793592Z","shell.execute_reply":"2023-11-29T12:03:01.800009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"VOCAB_MAP[\" \"]","metadata":{"id":"Qr2aeQNF7QGs","outputId":"2989c046-bbda-4dc4-c67b-996640029ccb","execution":{"iopub.status.busy":"2023-11-29T12:03:06.276768Z","iopub.execute_input":"2023-11-29T12:03:06.277477Z","iopub.status.idle":"2023-11-29T12:03:06.283024Z","shell.execute_reply.started":"2023-11-29T12:03:06.277444Z","shell.execute_reply":"2023-11-29T12:03:06.282126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SpeechDatasetME(torch.utils.data.Dataset): # Memory efficient\n    # Loades the data in get item to save RAM\n\n    def __init__(self, root, partition= \"train-clean-360\", transforms = None, cepstral=True):\n\n        self.VOCAB      = VOCAB\n        self.cepstral   = cepstral\n\n        if partition == \"train-clean-100\" or partition == \"train-clean-360\":\n            mfcc_dir       = str(f\"{root}{partition}/mfcc/\")# path to the mfccs\n            transcript_dir = str(f\"{root}{partition}/transcripts/\")# path to the transcripts\n\n            mfcc_files          = [mfcc_dir+i for i in sorted(os.listdir(mfcc_dir))]# create a list of paths for all the mfccs in the mfcc directory\n            transcript_files    = [transcript_dir+i for i in sorted(os.listdir(transcript_dir))]# create a list of paths for all the transcripts in the transcript directory\n\n        else:\n            mfcc_dir       = str(f\"{root}train-clean-100/mfcc/\")# path to the mfccs in the train clean 100 partition\n            transcript_dir = str(f\"{root}train-clean-100/transcripts/\")# path to the transcripts in the train clean 100 partition\n\n            mfcc_files          = [mfcc_dir+i for i in sorted(os.listdir(mfcc_dir))]# create a list of paths for all the mfccs in the mfcc directory\n            transcript_files    = [transcript_dir+i for i in sorted(os.listdir(transcript_dir))]# create a list of paths for all the transcripts in the transcript directory\n\n            mfcc_dir       = str(f\"{root}train-clean-360/mfcc/\")# path to the mfccs in the train clean 360 partition\n            transcript_dir = str(f\"{root}train-clean-360/transcripts/\")# path to the transcripts in the train clean 100 partition\n\n            # add the list of mfcc and transcript paths from train-clean-360 to the list of paths  from train-clean-100\n            # TODO\n\n            mfcc_files          += [mfcc_dir+i for i in sorted(os.listdir(mfcc_dir))]# create a list of paths for all the mfccs in the mfcc directory\n            transcript_files    += [transcript_dir+i for i in sorted(os.listdir(transcript_dir))]# create a list of paths for all the transcripts in the transcript directory\n\n\n        assert len(mfcc_files) == len(transcript_files)\n        length = len(mfcc_files)# TODO\n\n        self.mfcc_files         = mfcc_files\n        self.transcript_files   = transcript_files\n        self.length             = len(transcript_files)\n        print(\"Loaded file paths ME: \", partition)\n\n\n    def __len__(self):\n        return self.length# TODO\n\n    def __getitem__(self, ind):\n\n        # Load the mfcc and transcripts from the mfcc and transcript paths created earlier\n        mfcc        = np.load(self.mfcc_files[ind])# TODO\n        transcript  = np.load(self.transcript_files[ind])# TODO\n\n        # Normalize the mfccs and map the transcripts to integers\n        mfcc                = (mfcc - mfcc.mean(axis=0))/mfcc.std(axis=0)# TODO\n        transcript_mapped   = [VOCAB_MAP[trans_idx] for trans_idx in transcript]# TODO\n\n        return torch.FloatTensor(mfcc), torch.LongTensor(transcript_mapped)\n\n    def collate_fn(self,batch):\n\n        batch_x, batch_y, lengths_x, lengths_y = [], [], [], []\n\n        for x, y in batch:\n            # Add the mfcc, transcripts and their lengths to the lists created above\n            # TODO\n            batch_x.append(x)\n            batch_y.append(y)\n            lengths_x.append(len(x))\n            lengths_y.append(len(y))\n\n        # pack the mfccs and transcripts using the pad_sequence function from pytorch\n        batch_x_pad = pad_sequence(batch_x, batch_first=True)# TODO\n        batch_y_pad = pad_sequence(batch_y, batch_first=True)# TODO\n\n        return batch_x_pad, batch_y_pad, torch.tensor(lengths_x), torch.tensor(lengths_y)","metadata":{"id":"VuneWaTStdF2","execution":{"iopub.status.busy":"2023-11-29T12:24:06.590120Z","iopub.execute_input":"2023-11-29T12:24:06.590758Z","iopub.status.idle":"2023-11-29T12:24:06.654914Z","shell.execute_reply.started":"2023-11-29T12:24:06.590721Z","shell.execute_reply":"2023-11-29T12:24:06.653649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SpeechDatasetTest(torch.utils.data.Dataset):\n\n    def __init__(self, root, partition, cepstral=False):\n\n        self.mfcc_dir   = str(f\"{root}{partition}/mfcc/\")# path to the test-clean mfccs\n        self.mfcc_files = sorted(os.listdir(self.mfcc_dir))# list files in the mfcc directory\n\n        self.mfccs = []\n        for i, filename in enumerate(blue_tqdm(self.mfcc_files)):\n            mfcc = np.load(self.mfcc_dir+filename)# load the mfccs\n            if cepstral:\n                # Normalize the mfccs\n                mfcc = (mfcc - mfcc.mean(axis=0))/mfcc.std(axis=0)# TODO\n            # append the mfcc to the mfcc list created earlier\n            self.mfccs.append(mfcc)\n\n        self.length = len(self.mfccs)\n        print(\"Loaded: \", partition)\n\n    def __len__(self):\n        # TODO\n        return self.length\n    def __getitem__(self, ind):\n        # TODO\n        return torch.FloatTensor(self.mfccs[ind])\n\n    def collate_fn(self,batch):\n\n        batch_x, lengths_x = [], []\n        for x in batch:\n            # Append the mfccs and their lengths to the lists created above\n            batch_x.append(x)\n            lengths_x.append(len(x))\n        # pack the mfccs using the pad_sequence function from pytorch\n        batch_x_pad = pad_sequence(batch_x, batch_first=True)# TODO\n\n        return batch_x_pad, torch.tensor(lengths_x)","metadata":{"id":"jUrqTkG4VZfJ","execution":{"iopub.status.busy":"2023-11-29T12:08:14.369129Z","iopub.execute_input":"2023-11-29T12:08:14.369513Z","iopub.status.idle":"2023-11-29T12:08:14.380195Z","shell.execute_reply.started":"2023-11-29T12:08:14.369485Z","shell.execute_reply":"2023-11-29T12:08:14.379260Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATA_DIR        = '/kaggle/input/attention-based-speech-recognition/11-785-f23-hw4p2/'\nPARTITION       = config['train_dataset']\nCEPSTRAL        = config['cepstral_norm']\n\ntrain_dataset   = SpeechDatasetME( # Or AudioDatasetME\n    root        = DATA_DIR,\n    partition   = PARTITION,\n    cepstral    = CEPSTRAL\n)\nvalid_dataset   = SpeechDatasetME(\n    root        = DATA_DIR,\n    partition   = 'dev-clean',\n    cepstral    = CEPSTRAL\n)\ntest_dataset    = SpeechDatasetTest(\n    root        = DATA_DIR,\n    partition   = 'test-clean',\n    cepstral    = CEPSTRAL,\n)\n\ngc.collect()","metadata":{"id":"rsl5Q1jLvsOL","outputId":"9c519ce3-1f8c-40fc-bce1-d9bad555cc50","execution":{"iopub.status.busy":"2023-11-29T12:14:32.926280Z","iopub.execute_input":"2023-11-29T12:14:32.927218Z","iopub.status.idle":"2023-11-29T12:14:32.985254Z","shell.execute_reply.started":"2023-11-29T12:14:32.927180Z","shell.execute_reply":"2023-11-29T12:14:32.984072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader    = torch.utils.data.DataLoader(\n    dataset     = train_dataset,\n    batch_size  = config['batch_size'],\n    shuffle     = True,\n    num_workers = 2,\n    pin_memory  = True,\n    collate_fn  = train_dataset.collate_fn\n)\n\nvalid_loader    = torch.utils.data.DataLoader(\n    dataset     = valid_dataset,\n    batch_size  = config['batch_size'],\n    shuffle     = False,\n    num_workers = 2,\n    pin_memory  = True,\n    collate_fn  = valid_dataset.collate_fn\n)\n\ntest_loader     = torch.utils.data.DataLoader(\n    dataset     = test_dataset,\n    batch_size  = config['batch_size'],\n    shuffle     = False,\n    num_workers = 2,\n    pin_memory  = True,\n    collate_fn  = test_dataset.collate_fn\n)\n\nprint(\"No. of train mfccs   : \", train_dataset.__len__())\nprint(\"Batch size           : \", config['batch_size'])\nprint(\"Train batches        : \", train_loader.__len__())\nprint(\"Valid batches        : \", valid_loader.__len__())\nprint(\"Test batches         : \", test_loader.__len__())","metadata":{"id":"OeqXHogpwFfa","outputId":"1235447a-3a9c-4a1a-e573-6f4179748f26"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!sudo umount /dev/shm/ && sudo mount -t tmpfs -o rw,nosuid,nodev,noexec,relatime,size=5G shm /dev/shm","metadata":{"id":"umMU52yqg5yc","outputId":"de653930-b1f0-4801-d71b-e827c3523f69"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!sudo mount -t tmpfs -o rw,nosuid,nodev,noexec,relatime,size=5G shm /dev/shm","metadata":{"id":"_8tIQ0H5hh1X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\nChecking the shapes of the data...\")\nfor batch in train_loader:\n    x, y, x_len, y_len = batch\n    print(x.shape, y.shape, x_len.shape, y_len.shape)\n    print(y)\n    break","metadata":{"id":"tzuIXCyAuNvo","outputId":"e31b1a74-1dd7-4fb8-df5c-5d47f8dac89b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# y[0]","metadata":{"id":"91_d1JmeCf-7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def verify_dataset(dataset, partition= 'train-clean-100'):\n    print(\"\\nPartition loaded     : \", partition)\n    if partition != 'test-clean':\n        print(\"Max mfcc length          : \", np.max([data[0].shape[0] for data in dataset]))\n        print(\"Avg mfcc length          : \", np.mean([data[0].shape[0] for data in dataset]))\n        print(\"Max transcript length    : \", np.max([data[1].shape[0] for data in dataset]))\n        print(\"Max transcript length    : \", np.mean([data[1].shape[0] for data in dataset]))\n    else:\n        print(\"Max mfcc length          : \", np.max([data.shape[0] for data in dataset]))\n        print(\"Avg mfcc length          : \", np.mean([data.shape[0] for data in dataset]))\n\nverify_dataset(train_dataset, partition= 'train-clean-100')\nverify_dataset(valid_dataset, partition= 'dev-clean')\nverify_dataset(test_dataset, partition= 'test-clean')\ndataset_max_len  = max(\n    np.max([data[0].shape[0] for data in train_dataset]),\n    np.max([data[0].shape[0] for data in valid_dataset]),\n    np.max([data.shape[0] for data in test_dataset])\n)\nprint(\"\\nMax Length: \", dataset_max_len)","metadata":{"id":"8QFdYrM7xcI1","outputId":"d1ef3cf7-a0a7-4335-f0b8-c5f5c848e7a7","scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check if you are loading the data correctly with the following:\n\n- Train Dataset\n```\nPartition loaded:  train-clean-100\nMax mfcc length:  2448\nAverage mfcc length:  1264.6258453344547\nMax transcript:  400\nAverage transcript length:  186.65321139493324\n```\n\n- Dev Dataset\n```\nPartition loaded:  dev-clean\nMax mfcc length:  3260\nAverage mfcc length:  713.3570107288198\nMax transcript:  518\nAverage transcript length:  108.71698113207547\n```\n\n- Test Dataset\n```\nPartition loaded:  test-clean\nMax mfcc length:  3491\nAverage mfcc length:  738.2206106870229\n```\n\nIf your values is not matching, read hints, think what could have gone wrong. Then approach TAs.","metadata":{"id":"i_n3pqt7ud4t"}},{"cell_type":"markdown","source":"# THE MODEL\n\n### Listen, Attend and Spell\nListen, Attend and Spell (LAS) is a neural network model used for speech recognition and synthesis tasks.\n\n- LAS is designed to handle long input sequences and is robust to noisy speech signals.\n- LAS is known for its high accuracy and ability to improve over time with additional training data.\n- It consists of an <b>listener, an attender and a speller</b>, which work together to convert an input speech signal into a corresponding output text.\n\n#### The Dataflow:\n<center>\n<img src=\"https://github.com/varunjain3/11785_s23_h4p2/raw/main/DataFlow.png\" alt=\"data flow\" height=\"100\">\n</center>\n\n#### The Listener:\n- converts the input speech signal into a sequence of hidden states.\n\n#### The Attender:\n- Decides how the sequence of Encoder hidden state is propogated to decoder.\n\n#### The Speller:\n- A language model, that incorporates the \"context of attender\"(output of attender) to predict sequence of words.\n\n\n\n\n","metadata":{"id":"M8q9wt4TwzPt"}},{"cell_type":"markdown","source":"## Utils\n","metadata":{"id":"wTZ-lv47XOj0"}},{"cell_type":"code","source":"class PermuteBlock(torch.nn.Module):\n    def forward(self, x):\n        return x.transpose(1, 2)\n\ndef plot_attention(attention):\n    # Function for plotting attention\n    # You need to get a diagonal plot\n    plt.clf()\n    seaborn.heatmap(attention, cmap='GnBu')\n    plt.show()\n\ndef save_model(model, optimizer, scheduler, tf_scheduler, metric, epoch, path):\n    torch.save(\n        {'model_state_dict'         : model.state_dict(),\n         'optimizer_state_dict'     : optimizer.state_dict(),\n         'scheduler_state_dict'     : scheduler.state_dict(),\n         'tf_scheduler'             : tf_scheduler,\n         metric[0]                  : metric[1],\n         'epoch'                    : epoch},\n         path\n    )\n\ndef load_model(best_path, epoch_path, model, mode= 'best', metric= 'valid_acc', optimizer= None, scheduler= None, tf_scheduler= None):\n\n\n    if mode == 'best':\n        checkpoint  = torch.load(best_path)\n        print(\"Loading best checkpoint: \", checkpoint[metric])\n    else:\n        checkpoint  = torch.load(epoch_path)\n        print(\"Loading epoch checkpoint: \", checkpoint[metric])\n\n    model.load_state_dict(checkpoint['model_state_dict'], strict= False)\n\n    if optimizer != None:\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        #optimizer.param_groups[0]['lr'] = 1.5e-3\n        optimizer.param_groups[0]['weight_decay'] = 1e-5\n    if scheduler != None:\n        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n    if tf_scheduler != None:\n        tf_scheduler    = checkpoint['tf_scheduler']\n\n    epoch   = checkpoint['epoch']\n    metric  = torch.load(best_path)[metric]\n\n    return [model, optimizer, scheduler, tf_scheduler, epoch, metric]\n\nclass TimeElapsed():\n    def __init__(self):\n        self.start  = -1\n\n    def time_elapsed(self):\n        if self.start == -1:\n            self.start = time.time()\n        else:\n            end = time.time() - self.start\n            hrs, rem    = divmod(end, 3600)\n            min, sec    = divmod(rem, 60)\n            min         = min + 60*hrs\n            print(\"Time Elapsed: {:0>2}:{:02}\".format(int(min),int(sec)))\n            self.start  = -1","metadata":{"id":"FuRzPOaX0EtJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modules","metadata":{"id":"YiUnK0GMXTY6"}},{"cell_type":"markdown","source":"# Transformer Encoder","metadata":{"id":"nUQUwEHmCxeI"}},{"cell_type":"code","source":"torch.arange(0, 10, 2).unsqueeze(1)","metadata":{"id":"WQLJMwfOO4vh","outputId":"fdf892d6-9ce0-41cb-93fd-b199a02dc223"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\n\n# class PositionalEncoding(torch.nn.Module):\n\n#     def __init__(self, projection_size, max_seq_len= 176):\n#         super().__init__()\n#         # Read the Attention Is All You Need paper to learn how to code code the positional encoding\n\n#         # TODO\n\n#     def forward(self, x):\n#         # TODO\n\nclass PositionalEncoding(torch.nn.Module):\n    def __init__(self, projection_size, max_seq_len=4000):\n        super().__init__()\n        self.dropout = torch.nn.Dropout(p=0.1)\n        self.projection_size = projection_size\n        self.max_seq_len = max_seq_len\n\n        # Calculate the positional encoding matrix\n        pe = torch.zeros(max_seq_len, projection_size)\n        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, projection_size, 2).float() * (-math.log(10000.0) / projection_size))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        # Add the positional encoding to the input tensor\n        # x = x * math.sqrt(self.projection_size)\n        seq_len = x.size(1)\n        x = x + self.pe[:,:seq_len, :]\n        return self.dropout(x)\n\n\n\n# class TransformerEncoder(torch.nn.Module):\n#     def __init__(self, projection_size, num_heads, dropout= 0.0):\n#         super().__init__()\n\n#         # create the key, query and value weights\n#         self.KW         = # TODO\n#         self.VW         = # TODO\n#         self.QW         = # TODO\n\n#         self.permute    = PermuteBlock()\n\n#         # Compute multihead attention. You are free to use the version provided by pytorch\n#         self.attention  = # TODO\n\n#         self.bn1        = # TODO\n\n#         self.bn2        = # TODO\n\n#         # Feed forward neural network\n#         self.MLP        = # TODO\n\n#     def forward(self, x):\n#         # compute the key, query and value\n#         key     = # TODO\n#         value   = # TODO\n#         query   = # TODO\n\n#         # compute the output of the attention module\n#         out1    = # TODO\n#         # Create a residual connection between the input and the output of the attention module\n#         out1    = # TODO\n#         # Apply batch norm to out1\n#         out1    = # TODO\n\n#         # Apply the output of the feed forward network\n#         out2    = # TODO\n#         # Apply a residual connection between the input and output of the  FFN\n#         out2    = # TODO\n#         # Apply batch norm to the output\n#         out2    = # TODO\n\n#         return out2\n\n\nclass TransformerEncoder(torch.nn.Module):\n    def __init__(self, projection_size, num_heads, dropout=0.0):\n        super().__init__()\n\n        # create the key, query and value weights\n        self.KW = torch.nn.Linear(projection_size, projection_size)\n        self.VW = torch.nn.Linear(projection_size, projection_size)\n        self.QW = torch.nn.Linear(projection_size, projection_size)\n\n        self.permute    = PermuteBlock() # we do not know what it is doing here.\n\n        # Compute multihead attention. You are free to use the version provided by pytorch\n        self.attention = torch.nn.MultiheadAttention(projection_size, num_heads, dropout=dropout)\n\n        self.bn1 = torch.nn.BatchNorm1d(projection_size)\n        self.bn2 = torch.nn.BatchNorm1d(projection_size)\n\n        # Feed forward neural network\n        self.MLP = torch.nn.Sequential(\n            torch.nn.Linear(projection_size, projection_size),\n            torch.nn.ReLU(),\n            torch.nn.Linear(projection_size, projection_size)\n        )\n\n        # self.pe = PositionalEncoding(projection_size)\n\n    def forward(self, x):\n        # Add positional encoding to the input tensor\n        # x = self.pe(x)\n\n        # compute the key, query and value\n        key = self.KW(x)\n        value = self.VW(x)\n        query = self.QW(x)\n\n        # compute the output of the attention module\n        out1, _ = self.attention(query, key, value)\n        # Create a residual connection between the input and the output of the attention module\n\n        out1 = x + out1\n        # Apply batch norm to out1\n\n        out1 = self.bn1(out1.transpose(1,2))\n        out1 = out1.transpose(1,2)\n\n        # Apply the output of the feed forward network\n        out2 = self.MLP(out1)\n        # Apply a residual connection between the input and output of the  FFN\n        out2 = out1 + out2\n        # Apply batch norm to the output\n        out2 = self.bn2(out2.transpose(1,2))\n        out2 = out2.transpose(1,2)\n        # print(\"x.shape:\",x.shape,\"\\nout2.shape:\",out2.shape )\n        return out2\n\n\nmodel   = TransformerEncoder(\n    projection_size  = 128, num_heads=4\n).to(DEVICE)\n\n\nprint(model)\n\nx_sample    = torch.rand(128, 176, 256)\n# torchsummaryX.summary(model, x_sample.to(DEVICE))\ndel x_sample","metadata":{"id":"pLVW4Qw1C06t","outputId":"900eed9d-bfc8-4a29-89dc-97f0d91e4b95","scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class pBLSTM(torch.nn.Module):\n\n    def __init__(self, input_size, hidden_size, num_layers=1):\n        super(pBLSTM, self).__init__()\n\n        self.blstm = nn.LSTM(input_size =input_size*2, hidden_size = hidden_size,\n                             num_layers = num_layers,\n                             batch_first = True, dropout = 0.18, bidirectional = True) # TODO: Initialize a single layer bidirectional LSTM with the given input_size and hidden_size\n\n\n    def forward(self, x_packed):\n        x, seq_length = pad_packed_sequence(x_packed, batch_first = True, padding_value = PAD_TOKEN)\n        x_downsampled, x_lens = self.trunc_reshape(x, seq_length)\n        x_down_packed = pack_padded_sequence(x_downsampled, x_lens, batch_first = True, enforce_sorted = False)\n        output, _ = self.blstm(x_down_packed)\n        return output, x_lens\n\n    def trunc_reshape(self, x, x_lens):\n        batch_size, t_step, feature_dim = x.shape\n\n        if t_step % 2 != 0:\n          x = x[:,:-1,:]\n          t_step -= 1\n\n        x = x.reshape((batch_size, int(t_step/2), feature_dim*2))\n        x_lens = x_lens//2\n        return x, x_lens","metadata":{"id":"xjQpv3GOYkcu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TransformerListener(torch.nn.Module):\n\n    def __init__(self,\n                 input_size,\n                 base_lstm_layers        = 1,\n                 pblstm_layers           = 1,\n                 listener_hidden_size    = 256,\n                 n_heads                 = 8,\n                 tf_blocks               = 1):\n        super().__init__()\n\n        # create an lstm layer\n        self.base_lstm      = pBLSTM(input_size = input_size,\n                                     num_layers = base_lstm_layers,\n                                     hidden_size = listener_hidden_size)# TODO\n\n        # create a sequence of Conv1d layers\n        self.embedding      = torch.nn.Sequential(\n              torch.nn.Conv1d(in_channels=listener_hidden_size*2,out_channels=listener_hidden_size,kernel_size=5,padding=2),\n              torch.nn.BatchNorm1d(listener_hidden_size),\n              torch.nn.GELU(),\n              torch.nn.Conv1d(in_channels=listener_hidden_size,out_channels= listener_hidden_size,kernel_size=5,padding=2)\n              )# TODO\n\n        # compute the postion encoding\n        self.positional_encoding    = PositionalEncoding(projection_size=listener_hidden_size)# TODO\n\n        # create a sequence of transformer blocks\n        self.transformer_encoder    = torch.nn.Sequential(\n            *[TransformerEncoder(projection_size=listener_hidden_size, num_heads =  n_heads) for _ in range(tf_blocks)]\n        )\n        # for i in range(tf_blocks):\n        #     # TODO\n\n    def forward(self, x, x_len):\n\n        # pack the inputs before passing them to the LSTm\n        x_packed                = pack_padded_sequence(x,x_len, batch_first=True, enforce_sorted = False) # TODO\n        # Pass the packed sequence through the lstm\n        lstm_out, _             = self.base_lstm(x_packed)# TODO\n        # Unpack the output of the lstm\n        output, output_lengths  = pad_packed_sequence(lstm_out, batch_first=True)# TODO\n\n        # Pass the output through the embedding\n        output                  = self.embedding(output.transpose(1,2)).transpose(1,2)# TODO\n        # calculate the new output length\n        output_lengths          = output_lengths//2# TODO\n\n        # calculate the position encoding\n        output  = self.positional_encoding(output)# TODO\n        # Pass the output of the positional encoding through the transformer encoder\n        output  = self.transformer_encoder(output)# TODO\n\n        return output, output_lengths","metadata":{"id":"F0opYqry_EGi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Attention\n\n### Different ways to compute Attention\n\n1. Dot-product attention\n    * raw_weights = bmm(key, query)\n    * Optional: Scaled dot-product by normalizing with sqrt key dimension\n    * Check \"Attention is All You Need\" Section 3.2.1\n    * 1st way is what most TAs are comfortable with, but if you want to explore, check out other methods below\n\n\n2. Cosine attention\n    * raw_weights = cosine(query, key) # almost the same as dot-product xD\n\n3. Bi-linear attention\n    * W = Linear transformation (learnable parameter): d_k -> d_q\n    * raw_weights = bmm(key @ W, query)\n\n4. Multi-layer perceptron\n    * Check \"Neural Machine Translation and Sequence-to-sequence Models: A Tutorial\" Section 8.4\n\n5. Multi-Head Attention\n    * Check \"Attention is All You Need\" Section 3.2.2\n    * h = Number of heads\n    * W_Q, W_K, W_V: Weight matrix for Q, K, V (h of them in total)\n    * W_O: d_v -> d_v\n    * Reshape K: (B, T, d_k) to (B, T, h, d_k // h) and transpose to (B, h, T, d_k // h)\n    * Reshape V: (B, T, d_v) to (B, T, h, d_v // h) and transpose to (B, h, T, d_v // h)\n    * Reshape Q: (B, d_q) to (B, h, d_q // h) `\n    * raw_weights = Q @ K^T\n    * masked_raw_weights = mask(raw_weights)\n    * attention = softmax(masked_raw_weights)\n    * multi_head = attention @ V\n    * multi_head = multi_head reshaped to (B, d_v)\n    * context = multi_head @ W_O","metadata":{"id":"5fG9jDZBVklL"}},{"cell_type":"markdown","source":"Pseudocode:\n\n```python\nclass Attention:\n    '''\n    Attention is calculated using the key, value (from encoder embeddings) and query from decoder.\n\n    After obtaining the raw weights, compute and return attention weights and context as follows.:\n\n    attention_weights   = softmax(raw_weights)\n    attention_context   = einsum(\"thinkwhatwouldbetheequationhere\",attention, value) #take hint from raw_weights calculation\n\n    At the end, you can pass context through a linear layer too.\n    '''\n\n    def init(listener_hidden_size,\n              speller_hidden_size,\n              projection_size):\n\n        VW = Linear(listener_hidden_size,projection_size)\n        KW = Linear(listener_hidden_size,projection_size)\n        QW = Linear(speller_hidden_size,projection_size)\n\n    def set_key_value(encoder_outputs):\n        '''\n        In this function we take the encoder embeddings and make key and values from it.\n        key.shape   = (batch_size, timesteps, projection_size)\n        value.shape = (batch_size, timesteps, projection_size)\n        '''\n        key = KW(encoder_outputs)\n        value = VW(encoder_outputs)\n      \n    def compute_context(decoder_context):\n        '''\n        In this function from decoder context, we make the query, and then we\n         multiply the queries with the keys to find the attention logits,\n         finally we take a softmax to calculate attention energy which gets\n         multiplied to the generted values and then gets summed.\n\n        key.shape   = (batch_size, timesteps, projection_size)\n        value.shape = (batch_size, timesteps, projection_size)\n        query.shape = (batch_size, projection_size)\n\n        You are also recomended to check out Abu's Lecture 19 to understand Attention better.\n        '''\n        query = QW(decoder_context) #(batch_size, projection_size)\n\n        raw_weights = #using bmm or einsum. We need to perform batch matrix multiplication. It is important you do this step correctly.\n        #What will be the shape of raw_weights?\n\n        attention_weights = #What makes raw_weights -> attention_weights\n\n        attention_context = #Multiply attention weights to values\n\n        return attention_context, attention_weights\n```","metadata":{"id":"wyv0Q65t5SDd"}},{"cell_type":"code","source":"class Attention(torch.nn.Module):\n  def __init__(self,\n        listener_hidden_size,\n        speller_hidden_size,\n        projection_size):\n    super().__init__()\n    self.KW = torch.nn.Linear(listener_hidden_size, projection_size)\n    self.VW = torch.nn.Linear(listener_hidden_size, projection_size)\n    self.QW = torch.nn.Linear(listener_hidden_size, projection_size)\n\n    self.softmax = nn.Softmax()\n\n\n  def set_key_value(self, encoder_outputs):\n\n    self.key = self.KW(encoder_outputs)\n    self.value = self.VW(encoder_outputs)\n\n    # print(\"key: \", self.key.shape)\n    # print(\"value: \", self.value.shape)\n\n  def compute_context(self, decoder_context):\n    self.query = self.QW(decoder_context)\n    self.query = torch.unsqueeze(self.query,2)\n    length_q = self.query.shape[1]\n    # print(\"key: \", self.key.shape)\n    # print(\"q: \", self.query.shape)\n    raw_weights = torch.bmm(self.key, self.query).squeeze()\n    attention_weights = self.softmax(raw_weights/np.sqrt(length_q))\n    attention_context = torch.einsum('bi,bij->bj', attention_weights,self.value)\n\n    return attention_context, attention_weights","metadata":{"id":"771TXxn7ViOW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The Speller\n\nSimilar to the language model that you coded up for HW4P1, you have to code a language model for HW4P2 as well. This time, we will also call the attention context step, within the decoder to get the attended-encoder-embeddings.\n\n\nWhat you have coded till now:\n\n<center>\n<img src=\"https://github.com/varunjain3/11785_s23_h4p2/raw/main/EncoderAttention.png\" alt=\"data flow\" height=\"400\">\n</center>\n\nFor the Speller, what we have to code:\n\n\n<center>\n<img src=\"https://github.com/varunjain3/11785_s23_h4p2/raw/main/Decoder.png\" alt=\"data flow\" height=\"400\">\n</center>","metadata":{"id":"4Sp1WywZmm1L"}},{"cell_type":"code","source":"class Speller(torch.nn.Module):\n\n  # Refer to your HW4P1 implementation for help with setting up the language model.\n  # The only thing you need to implement on top of your HW4P1 model is the attention module and teacher forcing.\n\n  def __init__(self,vocab_size, embedding_size, projection_size, speller_size,  attender:Attention):\n    super(). __init__()\n\n    self.projection_size = projection_size\n    self.attend = attender # Attention object in speller\n    self.max_timesteps = 590# Max timesteps\n\n    lstm_size = embedding_size+projection_size\n    self.embedding =  torch.nn.Embedding(vocab_size, embedding_size) # Embedding layer to convert token to latent space\n    self.lstm_cells =  torch.nn.Sequential(\n            torch.nn.LSTMCell(lstm_size, speller_size),\n            torch.nn.LSTMCell(speller_size, speller_size),\n            torch.nn.LSTMCell(speller_size, speller_size)\n        )# Create a sequence of LSTM Cells\n\n    # For CDN (Feel free to change)\n    self.output_to_char = torch.nn.Linear(speller_size+projection_size, embedding_size)# Linear module to convert outputs to correct hidden size (Optional: TO make dimensions match)\n    self.activation = torch.nn.GELU()# Check which activation is suggested\n    self.char_prob = torch.nn.Linear(embedding_size, vocab_size)# Linear layer to convert hidden space back to logits for token classification\n    self.char_prob.weight = self.embedding.weight# Weight tying (From embedding layer)\n    self.drop = torch.nn.Dropout(p=0.21)\n\n  def lstm_step(self, input_word, hidden_state):\n\n    embedding =  input_word\n\n    for i in range(len(self.lstm_cells)):\n        embedding, cell_state = self.lstm_cells[i](embedding,hidden_state[i]) # Feed the input through each LSTM Cell\n\n\n        embedding = self.drop(embedding)\n        hidden_state[i] = (embedding, cell_state)\n\n    return embedding, hidden_state # What information does forward() need?\n\n\n  def CDN(self,input):\n    # Make the CDN here, you can add the output-to-char\n    # raise NotImplementedError\n    out = self.activation(self.output_to_char(self.drop(input)))\n    # out = self.activation(self.linear(self.drop(out)))\n    out = self.char_prob(out)\n    return out\n\n  def forward (self, y=None, teacher_forcing_ratio=1, encoder_batch_size=x.shape[0]):\n\n    batch_size = encoder_batch_size #y.shape[0] #x.shape[0]\n    attn_context = torch.zeros(batch_size, self.projection_size).to(DEVICE)# initial context tensor for time t = 0\n    output_symbol = torch.full((batch_size,), SOS_TOKEN).to(DEVICE)# Set it to SOS for time t = 0\n    raw_outputs = []\n    attention_plot = []\n\n    if y is None:\n      timesteps = self.max_timesteps\n      teacher_forcing_ratio = 0 #Why does it become zero?\n\n    else:\n      timesteps = y.shape[1] # How many timesteps are we predicting for?\n\n    hidden_states_list = [None] * len(self.lstm_cells)# Initialize your hidden_states list here similar to HW4P1\n\n    for t in range(timesteps):\n      p = np.random.uniform(0,1)# generate a probability p between 0 and 1\n\n      if p < teacher_forcing_ratio and t > 0: # Why do we consider cases only when t > 0? What is considered when t == 0? Think.\n        output_symbol = y[:,t-1] # Take from y, else draw from probability distribution\n\n\n      char_embed = self.embedding(output_symbol) # Embed the character symbol\n\n      # Concatenate the character embedding and context from attention, as shown in the diagram\n      lstm_input = torch.cat((char_embed,attn_context), dim = 1)\n\n      # print(\"char_embed\",char_embed.shape)\n      # print(\"attn_context\",attn_context.shape)\n      # print(\"lstm_input\",lstm_input.shape)\n\n      lstm_out, hidden_states_list = self.lstm_step(lstm_input, hidden_states_list) # Feed the input through LSTM Cells and attention.\n      # What should we retrieve from forward_step to prepare for the next timestep?\n\n      # print(\"lstm_out\",lstm_out.shape)\n\n      attn_context, attn_weights = self.attend.compute_context(lstm_out) # Feed the resulting hidden state into attention\n\n      cdn_input = torch.cat((lstm_out,attn_context), dim = 1)# TODO: You need to concatenate the context from the attention module with the LSTM output hidden state, as shown in the diagram\n\n      raw_pred = self.CDN(cdn_input) # call CDN with cdn_input\n\n      # Generate a prediction for this timestep and collect it in output_symbols\n      output_symbol =  torch.argmax(raw_pred, dim = 1)# Draw correctly from raw_pred\n\n      raw_outputs.append(raw_pred) # for loss calculation\n      attention_plot.append(attn_weights) # for plotting attention plot\n\n\n    attention_plot = torch.stack(attention_plot, dim=1)\n    raw_outputs = torch.stack(raw_outputs, dim=1)\n\n    return raw_outputs, attention_plot","metadata":{"id":"nFkc6MbnlUPu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ASRModel(torch.nn.Module):\n  def __init__(self, vocab_size, embedding_size, input_size, encoder_hidden_size, listener_size, speller_size, projection_size, name=\"ASR\"): # add parameters\n    super().__init__()\n\n    # Pass the right parameters here\n    self.listener = TransformerListener(input_size, listener_hidden_size=encoder_hidden_size)\n    self.attend = Attention(listener_size, speller_size, projection_size)\n    self.speller = Speller(vocab_size, embedding_size, projection_size, speller_size, self.attend)\n    self.name = name\n\n  def forward(self, x,lx,y=None,teacher_forcing_ratio=1):\n    # Encode speech features\n    encoder_outputs, _ = self.listener(x,lx)\n\n    # We want to compute keys and values ahead of the decoding step, as they are constant for all timesteps\n    # Set keys and values using the encoder outputs\n    self.attend.set_key_value(encoder_outputs)\n\n    # Decode text with the speller using context from the attention\n    raw_outputs, attention_plots = self.speller(y=y,teacher_forcing_ratio=teacher_forcing_ratio,\n                                                encoder_batch_size = encoder_outputs.shape[0])\n\n    return raw_outputs, attention_plots","metadata":{"id":"scvB2cI-OSof"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Setup","metadata":{"id":"bPZD3vqdUisj"}},{"cell_type":"code","source":"torch.cuda.empty_cache()\ngc.collect()","metadata":{"id":"p2uX2P9YVnbk","outputId":"fa778278-8a37-4c95-8c6e-72e94a0ece96"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = ASRModel(\n\n    # Initialize your model\n    vocab_size=31,\n    embedding_size=580,\n    input_size=28,\n    encoder_hidden_size=256,\n    listener_size = 256,\n    speller_size = 256,\n    projection_size = 256\n)\n\nmodel = model.to(DEVICE)\nprint(model)","metadata":{"id":"a9LN0l5VUk_s","outputId":"e5e6003d-1899-48fd-fc1a-7584da3afd4c","scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loss Function, Optimizers, Scheduler","metadata":{"id":"23DMfXsaU6kj"}},{"cell_type":"code","source":"optimizer   = torch.optim.AdamW(model.parameters(), lr= config['learning_rate'])\n\ncriterion   = torch.nn.CrossEntropyLoss(reduction='mean',ignore_index=PAD_TOKEN)\n\nscaler      = torch.cuda.amp.GradScaler()\n\nscheduler   = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = 70, eta_min = 0.000001)\n","metadata":{"id":"216ukmHbU-ol"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Levenshtein Distance","metadata":{"id":"ZWQnB8lUVY4f"}},{"cell_type":"code","source":"# We have given you this utility function which takes a sequence of indices and converts them to a list of characters\ndef indices_to_chars(indices, vocab):\n    tokens = []\n    for i in indices: # This loops through all the indices\n        if int(i) == SOS_TOKEN: # If SOS is encountered, dont add it to the final list\n            continue\n        elif int(i) == EOS_TOKEN: # If EOS is encountered, stop the decoding process\n            break\n        else:\n            tokens.append(vocab[i])\n    return tokens\n\n# To make your life more easier, we have given the Levenshtein distantce / Edit distance calculation code\ndef calc_edit_distance(predictions, y, y_len, vocab= VOCAB, print_example= False):\n\n    dist                = 0\n    batch_size, seq_len = predictions.shape\n\n    for batch_idx in range(batch_size):\n\n        y_sliced    = indices_to_chars(y[batch_idx,0:y_len[batch_idx]], vocab)\n        pred_sliced = indices_to_chars(predictions[batch_idx], vocab)\n\n        # Strings - When you are using characters from the AudioDataset\n        y_string    = ''.join(y_sliced)\n        pred_string = ''.join(pred_sliced)\n\n        dist        += Levenshtein.distance(pred_string, y_string)\n        # Comment the above abd uncomment below for toy dataset\n        # dist      += Levenshtein.distance(y_sliced, pred_sliced)\n\n    if print_example:\n        # Print y_sliced and pred_sliced if you are using the toy dataset\n        print(\"\\nGround Truth : \", y_string)\n        print(\"Prediction   : \", pred_string)\n\n    dist    /= batch_size\n    return dist","metadata":{"id":"rSsiCdxPVeZW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train and Validation functions\n","metadata":{"id":"Pu4MrSMUUIyp"}},{"cell_type":"code","source":"def train(model, dataloader, criterion, optimizer, teacher_forcing_rate):\n\n    model.train()\n    batch_bar = blue_tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n\n    running_loss        = 0.0\n    running_perplexity  = 0.0\n\n    for i, (x, y, lx, ly) in enumerate(dataloader):\n        # if i < 1083: #for debugging purposes\n        #   continue\n\n        optimizer.zero_grad()\n\n        x, y, lx, ly = x.to(DEVICE), y.to(DEVICE), lx, ly\n\n        with torch.cuda.amp.autocast():\n\n            raw_predictions, attention_plot = model(x, lx, y = y, teacher_forcing_ratio= teacher_forcing_rate)\n\n            # Predictions are of Shape (batch_size, timesteps, vocab_size).\n            # Transcripts are of shape (batch_size, timesteps) Which means that you have batch_size amount of batches with timestep number of tokens.\n            # So in total, you have batch_size*timesteps amount of characters.\n            # Similarly, in predictions, you have batch_size*timesteps amount of probability distributions.\n            # How do you need to modify transcipts and predictions so that you can calculate the CrossEntropyLoss? Hint: Use Reshape/View and read the docs\n            # Also we recommend you plot the attention weights, you should get convergence in around 10 epochs, if not, there could be something wrong with\n            # your implementation\n\n\n            raw_predictions = torch.permute(raw_predictions, (0,2,1))\n            loss        =  criterion(raw_predictions, y)# TODO: Cross Entropy Loss\n\n            perplexity  = torch.exp(loss) # Perplexity is defined the exponential of the loss\n\n            running_loss        += loss.item()\n            running_perplexity  += perplexity.item()\n\n        # Backward on the masked loss\n        scaler.scale(loss).backward()\n\n        # Optional: Use torch.nn.utils.clip_grad_norm to clip gradients to prevent them from exploding, if necessary\n        # If using with mixed precision, unscale the Optimizer First before doing gradient clipping\n\n        scaler.step(optimizer)\n        scaler.update()\n\n\n        batch_bar.set_postfix(\n            loss=\"{:.04f}\".format(running_loss/(i+1)),\n            perplexity=\"{:.04f}\".format(running_perplexity/(i+1)),\n            lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])),\n            tf_rate='{:.02f}'.format(teacher_forcing_rate))\n        batch_bar.update()\n\n        del x, y, lx, ly\n        torch.cuda.empty_cache()\n\n    running_loss /= len(dataloader)\n    running_perplexity /= len(dataloader)\n    batch_bar.close()\n\n    return running_loss, running_perplexity, attention_plot","metadata":{"id":"gYRVKs9_2rsx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def validate(model, dataloader):\n\n    model.eval()\n\n    batch_bar = blue_tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc=\"Val\")\n\n    running_lev_dist = 0.0\n    my_count = 0\n\n    for i, (x, y, lx, ly) in enumerate(dataloader):\n\n        if i%500 != 0: #for debugging purposes\n          continue\n        my_count += 1\n        x, y, lx, ly = x.to(DEVICE), y.to(DEVICE), lx, ly\n\n        with torch.inference_mode():\n            raw_predictions, attentions = model(x, lx, y = None)\n\n        # Greedy Decoding\n        print(i,\"a\",end=\"\\r\")\n        greedy_predictions   =  torch.argmax(raw_predictions, dim=2)# TODO: How do you get the most likely character from each distribution in the batch?\n\n        print(i,\"a**********\",end=\"\\r\")\n        # Calculate Levenshtein Distance\n        running_lev_dist    += calc_edit_distance(greedy_predictions, y, ly, VOCAB, print_example = False) # You can use print_example = True for one specific index i in your batches if you want\n\n        batch_bar.set_postfix(\n            dist=\"{:.04f}\".format(running_lev_dist/(i+1)))\n        batch_bar.update()\n\n        del x, y, lx, ly\n        torch.cuda.empty_cache()\n\n    batch_bar.close()\n    running_lev_dist /= my_count #len(dataloader) # FOR DEBUGGING PURPOSES\n\n    return running_lev_dist","metadata":{"id":"uIx3tW7a2tze"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Wandb\n","metadata":{"id":"WhwhevgWQbDX"}},{"cell_type":"code","source":"# Login to Wandb\n# Initialize your Wandb Run Here\n# Save your model architecture in a txt file, and save the file to Wandb","metadata":{"id":"1Xbw_0eAQcoR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Experiment","metadata":{"id":"JmZhxhNseaIr"}},{"cell_type":"code","source":"best_lev_dist = float(\"inf\")\ntf_rate = 1.0\n\nfor epoch in range(0, config['epochs']):\n\n    print(\"\\nEpoch: {}/{}\".format(epoch+1, config['epochs']))\n\n    curr_lr = float(optimizer.param_groups[0]['lr'])\n    # Call train and validate, get attention weights from training\n\n    train_loss, running_perplexity, attention_plot = train(model,train_loader, criterion, optimizer,tf_rate)\n    valid_dist = validate(model, valid_loader)\n\n\n    # Print your metrics\n    print(\"\\tTrain Loss {:.04f}\\t Learning Rate {:.07f}\".format(train_loss, curr_lr))\n    print(\"\\tVal Dist {:.04f}\\t\".format(valid_dist))\n    print(\"\\tTeacher Forcing Ratio {}\\t\".format(tf_rate))\n\n\n    # Plot Attention for a single item in the batch\n    plot_attention(attention_plot[0].cpu().detach().numpy())\n\n    # Log metrics to Wandb\n\n    # wandb.log({\n    #     'train_loss': train_loss,\n    #     'valid_dist': valid_dist,\n    #     'lr'        : curr_lr,\n    #     'tf_ratio'  : tf_rate\n    # })\n    # Optional: Scheduler Step / Teacher Force Schedule Step\n\n    scheduler.step()\n\n    if valid_dist <= best_lev_dist:\n        best_lev_dist = valid_dist\n        # Save your model checkpoint here\n        save_path = \"Run3_model_{0}_{1}_{2}.pth\".format(config['learning_rate'],config['batch_size'],model.name)\n        save_model(model, optimizer, scheduler, \"tf_scheduler\",  ['valid_dist', valid_dist], epoch, save_path)\n        # (model, optimizer, scheduler, tf_scheduler, metric, epoch, path)\n        # wandb.save(save_path)\n        print(\"Saved best model\")","metadata":{"id":"JcTFu-AH3m4e","outputId":"f865ded1-0388-49e5-c544-3bd094842f3a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing","metadata":{"id":"hgFYFaBGeBqM"}},{"cell_type":"code","source":"# Optional: Load your best model Checkpoint here\n\n# TODO: Create a testing function similar to validation\n# TODO: Create a file with all predictions\n# TODO: Submit to Kaggle","metadata":{"id":"ndNCpcxkx2KG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def testing(model, dataloader):\n\n    results = []\n    model.eval()\n\n    # batch_bar = blue_tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc=\"Testing\")\n\n    running_lev_dist = 0.0\n\n    for i, (x,lx) in enumerate(dataloader):\n\n        x, lx = x.to(DEVICE), lx\n\n        with torch.inference_mode():\n            raw_predictions, attentions = model(x, lx, y = None)\n\n        greedy_predictions   = torch.argmax(raw_predictions, dim=2) # TODO: How do you get the most likely character from each distribution in the batch?\n\n        del x, lx\n        results.extend(greedy_predictions)\n    return results","metadata":{"id":"3sCc4TH6u6RJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint = torch.load(\"/content/Run3_model_0.001_150_LAS.pth\")\nmodel_dict = checkpoint[\"model_state_dict\"]\n# opt_dict = checkpoint[\"optimizer_state_dict\"]\n# sched_dict = checkpoint[\"scheduler_state_dict\"]\nmodel.load_state_dict(model_dict)\n# optimizer.load_state_dict(opt_dict)\n# scheduler.load_state_dict(sched_dict)\n","metadata":{"id":"pxhrfEe-u6RK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = testing(model, test_loader)","metadata":{"id":"leB7xhCmu6RK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# results","metadata":{"id":"-YkarOcSRyxE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('submission.csv', 'w') as file:\n  file.write(\"index,label\\n\")\n  for i, pred in enumerate(results):\n    pred_sliced = indices_to_chars(results[i], VOCAB)\n    pred_string = ''.join(pred_sliced)\n    file.write(f\"{i},{pred_string}\\n\")","metadata":{"id":"iYCk7DsWTbIs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!kaggle competitions submit -c attention-based-speech-recognition -f submission.csv -m \"initial sub\"","metadata":{"id":"_dSJNYDRu6RK","outputId":"52f9c7b7-1e97-4714-f3fa-a279c6a4c4e4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"AI-JfcyPUgWl"},"execution_count":null,"outputs":[]}]}
