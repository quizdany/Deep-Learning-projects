{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"widgets":{"application/vnd.jupyter.widget-state+json":{"073ff65d11eb47c493b2ea639226b0e8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e2219883740448acb5a2156285cf8c91","placeholder":"​","style":"IPY_MODEL_814e0a537755446f873ca1496fcbd530","value":"Train:   1%"}},"1911f6c9e3c94e2f8dab0d5bdaf09b1d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1e20bf164d564bb8b3cdb8d46cf722e6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"20f5274c56e346269144d86d18770793":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e20bf164d564bb8b3cdb8d46cf722e6","max":2620,"min":0,"orientation":"horizontal","style":"IPY_MODEL_506f6192973d426b8d9c66ea57aa9549","value":2620}},"3405ccac4c274c6f9a481164255bc433":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3fe3a033a7a8427a8ea38d85c15cc2d6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"506f6192973d426b8d9c66ea57aa9549":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"61f40d13cb114f5190cf81c68147f4da":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6510c6df7ed2402a9da0e5072627bb5d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6a8ba71260244df19c532e75912db8ca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"703fa46d6e8e49d786a819526f738489":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e47984c7bdd14b33a0800eb6f0ed4b9e","IPY_MODEL_20f5274c56e346269144d86d18770793","IPY_MODEL_8097df8147ee4d8a9090cd4037ae2248"],"layout":"IPY_MODEL_61f40d13cb114f5190cf81c68147f4da"}},"8097df8147ee4d8a9090cd4037ae2248":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3405ccac4c274c6f9a481164255bc433","placeholder":"​","style":"IPY_MODEL_3fe3a033a7a8427a8ea38d85c15cc2d6","value":" 2620/2620 [00:03&lt;00:00, 838.04it/s]"}},"814e0a537755446f873ca1496fcbd530":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a0b6b738542c41699873e13e379dc501":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_b6708535817a43d6bf968e06c5f96108","max":1084,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1911f6c9e3c94e2f8dab0d5bdaf09b1d","value":9}},"aa000c92712f4b1a83f81c3c68fdfbee":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e09f23a2682c4916a6f48606f520258e","placeholder":"​","style":"IPY_MODEL_ca58d9daa4ce43d0bcc1d943b5b965a1","value":" 9/1084 [00:14&lt;28:02,  1.56s/it, loss=3.0020, lr=0.0002, perplexity=20.2650, tf_rate=1.00]"}},"aec96d10e7014b809e757efe6a6e652f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_073ff65d11eb47c493b2ea639226b0e8","IPY_MODEL_a0b6b738542c41699873e13e379dc501","IPY_MODEL_aa000c92712f4b1a83f81c3c68fdfbee"],"layout":"IPY_MODEL_ccf14578a0184f20b58d3a8255754698"}},"b6708535817a43d6bf968e06c5f96108":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca58d9daa4ce43d0bcc1d943b5b965a1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ccf14578a0184f20b58d3a8255754698":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"e09f23a2682c4916a6f48606f520258e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e2219883740448acb5a2156285cf8c91":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e47984c7bdd14b33a0800eb6f0ed4b9e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6510c6df7ed2402a9da0e5072627bb5d","placeholder":"​","style":"IPY_MODEL_6a8ba71260244df19c532e75912db8ca","value":"100%"}}}},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":62741,"databundleVersionId":6820148,"sourceType":"competition"}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# HW4P2: Attention-based Speech Recognition\n\n<img src=\"https://cdn.shopify.com/s/files/1/0272/2080/3722/products/SmileBumperSticker_5400x.jpg\" alt=\"A cute cat\" width=\"600\">\n\n\nWelcome to the final assignment in 11785. In this HW, you will work on building a speech recognition system with <i>attention</i>. <br> <br>\n\n<center>\n<img src=\"https://popmn.org/wp-content/uploads/2020/03/pay-attention.jpg\" alt=\"A cute cat\" height=\"100\">\n</center>\n\nHW Writeup: [TODO] <br>\nKaggle Competition Link: https://www.kaggle.com/competitions/attention-based-speech-recognition <br>\nKaggle Dataset Link: https://www.kaggle.com/competitions/attention-based-speech-recognition/data\n<br>\nLAS Paper: https://arxiv.org/pdf/1508.01211.pdf <br>\nAttention is all you need:https://arxiv.org/pdf/1706.03762.pdf","metadata":{"id":"8XpNMS7Vk6Df"}},{"cell_type":"markdown","source":"# Read this section importantly!","metadata":{"id":"vwIdDTTmmZVe"}},{"cell_type":"markdown","source":"1. By now, we believe that you are already a great deep learning practitioner, Congratulations. 🎉\n\n2. You are allowed to use code from your previous homeworks for this homework. We will only provide, aspects that are necessary and new with this homework.\n\n3. There are a lot of resources provided in this notebook, that will help you check if you are running your implementations correctly.","metadata":{"id":"y9qsVrRemgh7"}},{"cell_type":"code","source":"!nvidia-smi","metadata":{"id":"8UK7J-dp5iN5","outputId":"3ebdf937-1517-43f7-e217-92f47aa9152d","execution":{"iopub.status.busy":"2023-11-29T12:01:03.451614Z","iopub.execute_input":"2023-11-29T12:01:03.452376Z","iopub.status.idle":"2023-11-29T12:01:04.425278Z","shell.execute_reply.started":"2023-11-29T12:01:03.452343Z","shell.execute_reply":"2023-11-29T12:01:04.424160Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Install some required libraries\n# Feel free to add more if you want\n!pip install -q python-levenshtein torchsummaryX wandb kaggle pytorch-nlp","metadata":{"id":"nYgaLmgy5iqR","outputId":"7db1a2f0-b17f-4c27-a1dc-bb8959932281","execution":{"iopub.status.busy":"2023-11-29T12:01:38.901158Z","iopub.execute_input":"2023-11-29T12:01:38.901577Z","iopub.status.idle":"2023-11-29T12:01:50.231090Z","shell.execute_reply.started":"2023-11-29T12:01:38.901538Z","shell.execute_reply":"2023-11-29T12:01:50.229962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torchaudio\nfrom torch import nn, Tensor\n# import torchsummary\n\nimport numpy as np\nimport os\n\nimport gc\nimport time\n\nimport pandas as pd\nfrom tqdm.notebook import tqdm as blue_tqdm\nimport matplotlib.pyplot as plt\nimport seaborn\nimport json\n\nimport math\nfrom typing import Optional, List\n\n\n#imports for decoding and distance calculation\ntry:\n    import wandb\n    import torchsummaryX\n    import Levenshtein\nexcept:\n    print(\"Didnt install some/all imports\")\n\nfrom torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(\"Device: \", DEVICE)","metadata":{"id":"0mkii-6Dsjr8","outputId":"5b1539ce-b4a3-4515-813d-9b288c8f79d8","execution":{"iopub.status.busy":"2023-11-29T12:01:58.121716Z","iopub.execute_input":"2023-11-29T12:01:58.122133Z","iopub.status.idle":"2023-11-29T12:01:58.130834Z","shell.execute_reply.started":"2023-11-29T12:01:58.122098Z","shell.execute_reply":"2023-11-29T12:01:58.130009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Config","metadata":{"id":"AIOBPQjzrx5n"}},{"cell_type":"code","source":"config = dict (\n    train_dataset       = 'train-clean-360', # train-clean-100, train-clean-360, train-clean-460\n    batch_size          = 96,\n    epochs              = 100,\n    learning_rate       = 2e-4,\n    weight_decay        = 5e-3,\n    cepstral_norm       = True,\n)","metadata":{"id":"EPchlig7rxia","execution":{"iopub.status.busy":"2023-11-29T12:02:06.091756Z","iopub.execute_input":"2023-11-29T12:02:06.092651Z","iopub.status.idle":"2023-11-29T12:02:06.097165Z","shell.execute_reply.started":"2023-11-29T12:02:06.092614Z","shell.execute_reply":"2023-11-29T12:02:06.096285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Kaggle Dataset Download","metadata":{"id":"-njBvl2Opd6I"}},{"cell_type":"code","source":"!pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\n!mkdir /root/.kaggle\n\nwith open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n    f.write('{\"username\":\"danielkwizera\",\"key\":\"858f28b48352b29cb72fa9f3815c1b5d\"}') # Put your kaggle username & key here\n\n!chmod 600 /root/.kaggle/kaggle.json","metadata":{"id":"PTyWR2sIp0Ns","outputId":"14fc963b-953e-4958-bf94-589da3a25f34","execution":{"iopub.status.busy":"2023-11-29T12:02:18.017667Z","iopub.execute_input":"2023-11-29T12:02:18.018053Z","iopub.status.idle":"2023-11-29T12:02:22.037487Z","shell.execute_reply.started":"2023-11-29T12:02:18.018023Z","shell.execute_reply":"2023-11-29T12:02:22.036200Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # # to download the dataset\n# !kaggle competitions download -c attention-based-speech-recognition\n\n# # # to unzip data quickly and quietly\n# !unzip -q attention-based-speech-recognition.zip -d ./data","metadata":{"id":"F581gjfnqE2C","outputId":"6127f379-46bd-44ab-a87f-28b36618d111","execution":{"iopub.status.busy":"2023-11-29T11:45:44.701508Z","iopub.execute_input":"2023-11-29T11:45:44.702468Z","iopub.status.idle":"2023-11-29T11:51:12.412597Z","shell.execute_reply.started":"2023-11-29T11:45:44.702430Z","shell.execute_reply":"2023-11-29T11:51:12.411374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Character-based LibriSpeech (HW4P2)\n\nIn terms of the dataset, the dataset structure for HW3P2 and HW4P2 dataset are very similar. Can you spot out the differences? What all will be required??\n\nHints:\n\n- Check how big is the dataset (do you require memory efficient loading techniques??)\n- How do we load mfccs? Do we need to normalise them?\n- Does the data have \\<SOS> and \\<EOS> tokens in each sequences? Do we remove them or do we not remove them? (Read writeup)\n- Would we want a collating function? Ask yourself: Why did we need a collate function last time?\n- Observe the VOCAB, is the dataset same as HW3P2?\n- Should you add augmentations, if yes which augmentations? When should you add augmentations? (Check bootcamp for answer)\n","metadata":{"id":"zUJyBBwIqQs6"}},{"cell_type":"code","source":"VOCAB = [\n    '<pad>', '<sos>', '<eos>',\n    'A',   'B',    'C',    'D',\n    'E',   'F',    'G',    'H',\n    'I',   'J',    'K',    'L',\n    'M',   'N',    'O',    'P',\n    'Q',   'R',    'S',    'T',\n    'U',   'V',    'W',    'X',\n    'Y',   'Z',    \"'\",    ' ',\n]\n\nVOCAB_MAP = {VOCAB[i]:i for i in range(0, len(VOCAB))}\n\nPAD_TOKEN = VOCAB_MAP[\"<pad>\"]\nSOS_TOKEN = VOCAB_MAP[\"<sos>\"]\nEOS_TOKEN = VOCAB_MAP[\"<eos>\"]\n\nprint(f\"Length of vocab : {len(VOCAB)}\")\nprint(f\"Vocab           : {VOCAB}\")\nprint(f\"PAD_TOKEN       : {PAD_TOKEN}\")\nprint(f\"SOS_TOKEN       : {SOS_TOKEN}\")\nprint(f\"EOS_TOKEN       : {EOS_TOKEN}\")","metadata":{"id":"MBMLGYX-kZcd","outputId":"7330ab8d-127b-486f-d33c-a59d4c0c22eb","execution":{"iopub.status.busy":"2023-11-29T12:02:58.637967Z","iopub.execute_input":"2023-11-29T12:02:58.638921Z","iopub.status.idle":"2023-11-29T12:02:58.646713Z","shell.execute_reply.started":"2023-11-29T12:02:58.638884Z","shell.execute_reply":"2023-11-29T12:02:58.645693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(VOCAB_MAP)","metadata":{"id":"9ERlUvyGCMuz","outputId":"2771c2a3-4f9c-448d-dcaa-6e58b9de06ea","execution":{"iopub.status.busy":"2023-11-29T12:03:01.793184Z","iopub.execute_input":"2023-11-29T12:03:01.793628Z","iopub.status.idle":"2023-11-29T12:03:01.801045Z","shell.execute_reply.started":"2023-11-29T12:03:01.793592Z","shell.execute_reply":"2023-11-29T12:03:01.800009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"VOCAB_MAP[\" \"]","metadata":{"id":"Qr2aeQNF7QGs","outputId":"2989c046-bbda-4dc4-c67b-996640029ccb","execution":{"iopub.status.busy":"2023-11-29T12:03:06.276768Z","iopub.execute_input":"2023-11-29T12:03:06.277477Z","iopub.status.idle":"2023-11-29T12:03:06.283024Z","shell.execute_reply.started":"2023-11-29T12:03:06.277444Z","shell.execute_reply":"2023-11-29T12:03:06.282126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SpeechDatasetME(torch.utils.data.Dataset): # Memory efficient\n    # Loades the data in get item to save RAM\n\n    def __init__(self, root, partition= \"train-clean-360\", transforms = None, cepstral=True):\n\n        self.VOCAB      = VOCAB\n        self.cepstral   = cepstral\n\n        if partition == \"train-clean-100\" or partition == \"train-clean-360\":\n            mfcc_dir       = str(f\"{root}{partition}/mfcc/\")# path to the mfccs\n            transcript_dir = str(f\"{root}{partition}/transcripts/\")# path to the transcripts\n\n            mfcc_files          = [mfcc_dir+i for i in sorted(os.listdir(mfcc_dir))]# create a list of paths for all the mfccs in the mfcc directory\n            transcript_files    = [transcript_dir+i for i in sorted(os.listdir(transcript_dir))]# create a list of paths for all the transcripts in the transcript directory\n\n        else:\n            mfcc_dir       = str(f\"{root}train-clean-100/mfcc/\")# path to the mfccs in the train clean 100 partition\n            transcript_dir = str(f\"{root}train-clean-100/transcripts/\")# path to the transcripts in the train clean 100 partition\n\n            mfcc_files          = [mfcc_dir+i for i in sorted(os.listdir(mfcc_dir))]# create a list of paths for all the mfccs in the mfcc directory\n            transcript_files    = [transcript_dir+i for i in sorted(os.listdir(transcript_dir))]# create a list of paths for all the transcripts in the transcript directory\n\n            mfcc_dir       = str(f\"{root}train-clean-360/mfcc/\")# path to the mfccs in the train clean 360 partition\n            transcript_dir = str(f\"{root}train-clean-360/transcripts/\")# path to the transcripts in the train clean 100 partition\n\n            # add the list of mfcc and transcript paths from train-clean-360 to the list of paths  from train-clean-100\n            # TODO\n\n            mfcc_files          += [mfcc_dir+i for i in sorted(os.listdir(mfcc_dir))]# create a list of paths for all the mfccs in the mfcc directory\n            transcript_files    += [transcript_dir+i for i in sorted(os.listdir(transcript_dir))]# create a list of paths for all the transcripts in the transcript directory\n\n\n        assert len(mfcc_files) == len(transcript_files)\n        length = len(mfcc_files)# TODO\n\n        self.mfcc_files         = mfcc_files\n        self.transcript_files   = transcript_files\n        self.length             = len(transcript_files)\n        print(\"Loaded file paths ME: \", partition)\n\n\n    def __len__(self):\n        return self.length# TODO\n\n    def __getitem__(self, ind):\n\n        # Load the mfcc and transcripts from the mfcc and transcript paths created earlier\n        mfcc        = np.load(self.mfcc_files[ind])# TODO\n        transcript  = np.load(self.transcript_files[ind])# TODO\n\n        # Normalize the mfccs and map the transcripts to integers\n        mfcc                = (mfcc - mfcc.mean(axis=0))/mfcc.std(axis=0)# TODO\n        transcript_mapped   = [VOCAB_MAP[trans_idx] for trans_idx in transcript]# TODO\n\n        return torch.FloatTensor(mfcc), torch.LongTensor(transcript_mapped)\n\n    def collate_fn(self,batch):\n\n        batch_x, batch_y, lengths_x, lengths_y = [], [], [], []\n\n        for x, y in batch:\n            # Add the mfcc, transcripts and their lengths to the lists created above\n            # TODO\n            batch_x.append(x)\n            batch_y.append(y)\n            lengths_x.append(len(x))\n            lengths_y.append(len(y))\n\n        # pack the mfccs and transcripts using the pad_sequence function from pytorch\n        batch_x_pad = pad_sequence(batch_x, batch_first=True)# TODO\n        batch_y_pad = pad_sequence(batch_y, batch_first=True)# TODO\n\n        return batch_x_pad, batch_y_pad, torch.tensor(lengths_x), torch.tensor(lengths_y)","metadata":{"id":"VuneWaTStdF2","execution":{"iopub.status.busy":"2023-11-29T12:24:06.590120Z","iopub.execute_input":"2023-11-29T12:24:06.590758Z","iopub.status.idle":"2023-11-29T12:24:06.654914Z","shell.execute_reply.started":"2023-11-29T12:24:06.590721Z","shell.execute_reply":"2023-11-29T12:24:06.653649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SpeechDatasetTest(torch.utils.data.Dataset):\n\n    def __init__(self, root, partition, cepstral=False):\n\n        self.mfcc_dir   = str(f\"{root}{partition}/mfcc/\")# path to the test-clean mfccs\n        self.mfcc_files = sorted(os.listdir(self.mfcc_dir))# list files in the mfcc directory\n\n        self.mfccs = []\n        for i, filename in enumerate(blue_tqdm(self.mfcc_files)):\n            mfcc = np.load(self.mfcc_dir+filename)# load the mfccs\n            if cepstral:\n                # Normalize the mfccs\n                mfcc = (mfcc - mfcc.mean(axis=0))/mfcc.std(axis=0)# TODO\n            # append the mfcc to the mfcc list created earlier\n            self.mfccs.append(mfcc)\n\n        self.length = len(self.mfccs)\n        print(\"Loaded: \", partition)\n\n    def __len__(self):\n        # TODO\n        return self.length\n    def __getitem__(self, ind):\n        # TODO\n        return torch.FloatTensor(self.mfccs[ind])\n\n    def collate_fn(self,batch):\n\n        batch_x, lengths_x = [], []\n        for x in batch:\n            # Append the mfccs and their lengths to the lists created above\n            batch_x.append(x)\n            lengths_x.append(len(x))\n        # pack the mfccs using the pad_sequence function from pytorch\n        batch_x_pad = pad_sequence(batch_x, batch_first=True)# TODO\n\n        return batch_x_pad, torch.tensor(lengths_x)","metadata":{"id":"jUrqTkG4VZfJ","execution":{"iopub.status.busy":"2023-11-29T12:08:14.369129Z","iopub.execute_input":"2023-11-29T12:08:14.369513Z","iopub.status.idle":"2023-11-29T12:08:14.380195Z","shell.execute_reply.started":"2023-11-29T12:08:14.369485Z","shell.execute_reply":"2023-11-29T12:08:14.379260Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATA_DIR        = '/kaggle/input/attention-based-speech-recognition/11-785-f23-hw4p2/'\nPARTITION       = config['train_dataset']\nCEPSTRAL        = config['cepstral_norm']\n\ntrain_dataset   = SpeechDatasetME( # Or AudioDatasetME\n    root        = DATA_DIR,\n    partition   = PARTITION,\n    cepstral    = CEPSTRAL\n)\nvalid_dataset   = SpeechDatasetME(\n    root        = DATA_DIR,\n    partition   = 'dev-clean',\n    cepstral    = CEPSTRAL\n)\ntest_dataset    = SpeechDatasetTest(\n    root        = DATA_DIR,\n    partition   = 'test-clean',\n    cepstral    = CEPSTRAL,\n)\n\ngc.collect()","metadata":{"id":"rsl5Q1jLvsOL","outputId":"9c519ce3-1f8c-40fc-bce1-d9bad555cc50","execution":{"iopub.status.busy":"2023-11-29T12:14:32.926280Z","iopub.execute_input":"2023-11-29T12:14:32.927218Z","iopub.status.idle":"2023-11-29T12:14:32.985254Z","shell.execute_reply.started":"2023-11-29T12:14:32.927180Z","shell.execute_reply":"2023-11-29T12:14:32.984072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader    = torch.utils.data.DataLoader(\n    dataset     = train_dataset,\n    batch_size  = config['batch_size'],\n    shuffle     = True,\n    num_workers = 2,\n    pin_memory  = True,\n    collate_fn  = train_dataset.collate_fn\n)\n\nvalid_loader    = torch.utils.data.DataLoader(\n    dataset     = valid_dataset,\n    batch_size  = config['batch_size'],\n    shuffle     = False,\n    num_workers = 2,\n    pin_memory  = True,\n    collate_fn  = valid_dataset.collate_fn\n)\n\ntest_loader     = torch.utils.data.DataLoader(\n    dataset     = test_dataset,\n    batch_size  = config['batch_size'],\n    shuffle     = False,\n    num_workers = 2,\n    pin_memory  = True,\n    collate_fn  = test_dataset.collate_fn\n)\n\nprint(\"No. of train mfccs   : \", train_dataset.__len__())\nprint(\"Batch size           : \", config['batch_size'])\nprint(\"Train batches        : \", train_loader.__len__())\nprint(\"Valid batches        : \", valid_loader.__len__())\nprint(\"Test batches         : \", test_loader.__len__())","metadata":{"id":"OeqXHogpwFfa","outputId":"1235447a-3a9c-4a1a-e573-6f4179748f26"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!sudo umount /dev/shm/ && sudo mount -t tmpfs -o rw,nosuid,nodev,noexec,relatime,size=5G shm /dev/shm","metadata":{"id":"umMU52yqg5yc","outputId":"de653930-b1f0-4801-d71b-e827c3523f69"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!sudo mount -t tmpfs -o rw,nosuid,nodev,noexec,relatime,size=5G shm /dev/shm","metadata":{"id":"_8tIQ0H5hh1X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\nChecking the shapes of the data...\")\nfor batch in train_loader:\n    x, y, x_len, y_len = batch\n    print(x.shape, y.shape, x_len.shape, y_len.shape)\n    print(y)\n    break","metadata":{"id":"tzuIXCyAuNvo","outputId":"e31b1a74-1dd7-4fb8-df5c-5d47f8dac89b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# y[0]","metadata":{"id":"91_d1JmeCf-7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def verify_dataset(dataset, partition= 'train-clean-100'):\n    print(\"\\nPartition loaded     : \", partition)\n    if partition != 'test-clean':\n        print(\"Max mfcc length          : \", np.max([data[0].shape[0] for data in dataset]))\n        print(\"Avg mfcc length          : \", np.mean([data[0].shape[0] for data in dataset]))\n        print(\"Max transcript length    : \", np.max([data[1].shape[0] for data in dataset]))\n        print(\"Max transcript length    : \", np.mean([data[1].shape[0] for data in dataset]))\n    else:\n        print(\"Max mfcc length          : \", np.max([data.shape[0] for data in dataset]))\n        print(\"Avg mfcc length          : \", np.mean([data.shape[0] for data in dataset]))\n\nverify_dataset(train_dataset, partition= 'train-clean-100')\nverify_dataset(valid_dataset, partition= 'dev-clean')\nverify_dataset(test_dataset, partition= 'test-clean')\ndataset_max_len  = max(\n    np.max([data[0].shape[0] for data in train_dataset]),\n    np.max([data[0].shape[0] for data in valid_dataset]),\n    np.max([data.shape[0] for data in test_dataset])\n)\nprint(\"\\nMax Length: \", dataset_max_len)","metadata":{"id":"8QFdYrM7xcI1","outputId":"d1ef3cf7-a0a7-4335-f0b8-c5f5c848e7a7","scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check if you are loading the data correctly with the following:\n\n- Train Dataset\n```\nPartition loaded:  train-clean-100\nMax mfcc length:  2448\nAverage mfcc length:  1264.6258453344547\nMax transcript:  400\nAverage transcript length:  186.65321139493324\n```\n\n- Dev Dataset\n```\nPartition loaded:  dev-clean\nMax mfcc length:  3260\nAverage mfcc length:  713.3570107288198\nMax transcript:  518\nAverage transcript length:  108.71698113207547\n```\n\n- Test Dataset\n```\nPartition loaded:  test-clean\nMax mfcc length:  3491\nAverage mfcc length:  738.2206106870229\n```\n\nIf your values is not matching, read hints, think what could have gone wrong. Then approach TAs.","metadata":{"id":"i_n3pqt7ud4t"}},{"cell_type":"markdown","source":"# THE MODEL\n\n### Listen, Attend and Spell\nListen, Attend and Spell (LAS) is a neural network model used for speech recognition and synthesis tasks.\n\n- LAS is designed to handle long input sequences and is robust to noisy speech signals.\n- LAS is known for its high accuracy and ability to improve over time with additional training data.\n- It consists of an <b>listener, an attender and a speller</b>, which work together to convert an input speech signal into a corresponding output text.\n\n#### The Dataflow:\n<center>\n<img src=\"https://github.com/varunjain3/11785_s23_h4p2/raw/main/DataFlow.png\" alt=\"data flow\" height=\"100\">\n</center>\n\n#### The Listener:\n- converts the input speech signal into a sequence of hidden states.\n\n#### The Attender:\n- Decides how the sequence of Encoder hidden state is propogated to decoder.\n\n#### The Speller:\n- A language model, that incorporates the \"context of attender\"(output of attender) to predict sequence of words.\n\n\n\n\n","metadata":{"id":"M8q9wt4TwzPt"}},{"cell_type":"markdown","source":"## Utils\n","metadata":{"id":"wTZ-lv47XOj0"}},{"cell_type":"code","source":"class PermuteBlock(torch.nn.Module):\n    def forward(self, x):\n        return x.transpose(1, 2)\n\ndef plot_attention(attention):\n    # Function for plotting attention\n    # You need to get a diagonal plot\n    plt.clf()\n    seaborn.heatmap(attention, cmap='GnBu')\n    plt.show()\n\ndef save_model(model, optimizer, scheduler, tf_scheduler, metric, epoch, path):\n    torch.save(\n        {'model_state_dict'         : model.state_dict(),\n         'optimizer_state_dict'     : optimizer.state_dict(),\n         'scheduler_state_dict'     : scheduler.state_dict(),\n         'tf_scheduler'             : tf_scheduler,\n         metric[0]                  : metric[1],\n         'epoch'                    : epoch},\n         path\n    )\n\ndef load_model(best_path, epoch_path, model, mode= 'best', metric= 'valid_acc', optimizer= None, scheduler= None, tf_scheduler= None):\n\n\n    if mode == 'best':\n        checkpoint  = torch.load(best_path)\n        print(\"Loading best checkpoint: \", checkpoint[metric])\n    else:\n        checkpoint  = torch.load(epoch_path)\n        print(\"Loading epoch checkpoint: \", checkpoint[metric])\n\n    model.load_state_dict(checkpoint['model_state_dict'], strict= False)\n\n    if optimizer != None:\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        #optimizer.param_groups[0]['lr'] = 1.5e-3\n        optimizer.param_groups[0]['weight_decay'] = 1e-5\n    if scheduler != None:\n        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n    if tf_scheduler != None:\n        tf_scheduler    = checkpoint['tf_scheduler']\n\n    epoch   = checkpoint['epoch']\n    metric  = torch.load(best_path)[metric]\n\n    return [model, optimizer, scheduler, tf_scheduler, epoch, metric]\n\nclass TimeElapsed():\n    def __init__(self):\n        self.start  = -1\n\n    def time_elapsed(self):\n        if self.start == -1:\n            self.start = time.time()\n        else:\n            end = time.time() - self.start\n            hrs, rem    = divmod(end, 3600)\n            min, sec    = divmod(rem, 60)\n            min         = min + 60*hrs\n            print(\"Time Elapsed: {:0>2}:{:02}\".format(int(min),int(sec)))\n            self.start  = -1","metadata":{"id":"FuRzPOaX0EtJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modules","metadata":{"id":"YiUnK0GMXTY6"}},{"cell_type":"markdown","source":"# Transformer Encoder","metadata":{"id":"nUQUwEHmCxeI"}},{"cell_type":"code","source":"torch.arange(0, 10, 2).unsqueeze(1)","metadata":{"id":"WQLJMwfOO4vh","outputId":"fdf892d6-9ce0-41cb-93fd-b199a02dc223"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\n\n# class PositionalEncoding(torch.nn.Module):\n\n#     def __init__(self, projection_size, max_seq_len= 176):\n#         super().__init__()\n#         # Read the Attention Is All You Need paper to learn how to code code the positional encoding\n\n#         # TODO\n\n#     def forward(self, x):\n#         # TODO\n\nclass PositionalEncoding(torch.nn.Module):\n    def __init__(self, projection_size, max_seq_len=4000):\n        super().__init__()\n        self.dropout = torch.nn.Dropout(p=0.1)\n        self.projection_size = projection_size\n        self.max_seq_len = max_seq_len\n\n        # Calculate the positional encoding matrix\n        pe = torch.zeros(max_seq_len, projection_size)\n        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, projection_size, 2).float() * (-math.log(10000.0) / projection_size))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        # Add the positional encoding to the input tensor\n        # x = x * math.sqrt(self.projection_size)\n        seq_len = x.size(1)\n        x = x + self.pe[:,:seq_len, :]\n        return self.dropout(x)\n\n\n\n# class TransformerEncoder(torch.nn.Module):\n#     def __init__(self, projection_size, num_heads, dropout= 0.0):\n#         super().__init__()\n\n#         # create the key, query and value weights\n#         self.KW         = # TODO\n#         self.VW         = # TODO\n#         self.QW         = # TODO\n\n#         self.permute    = PermuteBlock()\n\n#         # Compute multihead attention. You are free to use the version provided by pytorch\n#         self.attention  = # TODO\n\n#         self.bn1        = # TODO\n\n#         self.bn2        = # TODO\n\n#         # Feed forward neural network\n#         self.MLP        = # TODO\n\n#     def forward(self, x):\n#         # compute the key, query and value\n#         key     = # TODO\n#         value   = # TODO\n#         query   = # TODO\n\n#         # compute the output of the attention module\n#         out1    = # TODO\n#         # Create a residual connection between the input and the output of the attention module\n#         out1    = # TODO\n#         # Apply batch norm to out1\n#         out1    = # TODO\n\n#         # Apply the output of the feed forward network\n#         out2    = # TODO\n#         # Apply a residual connection between the input and output of the  FFN\n#         out2    = # TODO\n#         # Apply batch norm to the output\n#         out2    = # TODO\n\n#         return out2\n\n\nclass TransformerEncoder(torch.nn.Module):\n    def __init__(self, projection_size, num_heads, dropout=0.0):\n        super().__init__()\n\n        # create the key, query and value weights\n        self.KW = torch.nn.Linear(projection_size, projection_size)\n        self.VW = torch.nn.Linear(projection_size, projection_size)\n        self.QW = torch.nn.Linear(projection_size, projection_size)\n\n        self.permute    = PermuteBlock() # we do not know what it is doing here.\n\n        # Compute multihead attention. You are free to use the version provided by pytorch\n        self.attention = torch.nn.MultiheadAttention(projection_size, num_heads, dropout=dropout)\n\n        self.bn1 = torch.nn.BatchNorm1d(projection_size)\n        self.bn2 = torch.nn.BatchNorm1d(projection_size)\n\n        # Feed forward neural network\n        self.MLP = torch.nn.Sequential(\n            torch.nn.Linear(projection_size, projection_size),\n            torch.nn.ReLU(),\n            torch.nn.Linear(projection_size, projection_size)\n        )\n\n        # self.pe = PositionalEncoding(projection_size)\n\n    def forward(self, x):\n        # Add positional encoding to the input tensor\n        # x = self.pe(x)\n\n        # compute the key, query and value\n        key = self.KW(x)\n        value = self.VW(x)\n        query = self.QW(x)\n\n        # compute the output of the attention module\n        out1, _ = self.attention(query, key, value)\n        # Create a residual connection between the input and the output of the attention module\n\n        out1 = x + out1\n        # Apply batch norm to out1\n\n        out1 = self.bn1(out1.transpose(1,2))\n        out1 = out1.transpose(1,2)\n\n        # Apply the output of the feed forward network\n        out2 = self.MLP(out1)\n        # Apply a residual connection between the input and output of the  FFN\n        out2 = out1 + out2\n        # Apply batch norm to the output\n        out2 = self.bn2(out2.transpose(1,2))\n        out2 = out2.transpose(1,2)\n        # print(\"x.shape:\",x.shape,\"\\nout2.shape:\",out2.shape )\n        return out2\n\n\nmodel   = TransformerEncoder(\n    projection_size  = 128, num_heads=4\n).to(DEVICE)\n\n\nprint(model)\n\nx_sample    = torch.rand(128, 176, 256)\n# torchsummaryX.summary(model, x_sample.to(DEVICE))\ndel x_sample","metadata":{"id":"pLVW4Qw1C06t","outputId":"900eed9d-bfc8-4a29-89dc-97f0d91e4b95","scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class pBLSTM(torch.nn.Module):\n\n    def __init__(self, input_size, hidden_size, num_layers=1):\n        super(pBLSTM, self).__init__()\n\n        self.blstm = nn.LSTM(input_size =input_size*2, hidden_size = hidden_size,\n                             num_layers = num_layers,\n                             batch_first = True, dropout = 0.18, bidirectional = True) # TODO: Initialize a single layer bidirectional LSTM with the given input_size and hidden_size\n\n\n    def forward(self, x_packed):\n        x, seq_length = pad_packed_sequence(x_packed, batch_first = True, padding_value = PAD_TOKEN)\n        x_downsampled, x_lens = self.trunc_reshape(x, seq_length)\n        x_down_packed = pack_padded_sequence(x_downsampled, x_lens, batch_first = True, enforce_sorted = False)\n        output, _ = self.blstm(x_down_packed)\n        return output, x_lens\n\n    def trunc_reshape(self, x, x_lens):\n        batch_size, t_step, feature_dim = x.shape\n\n        if t_step % 2 != 0:\n          x = x[:,:-1,:]\n          t_step -= 1\n\n        x = x.reshape((batch_size, int(t_step/2), feature_dim*2))\n        x_lens = x_lens//2\n        return x, x_lens","metadata":{"id":"xjQpv3GOYkcu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TransformerListener(torch.nn.Module):\n\n    def __init__(self,\n                 input_size,\n                 base_lstm_layers        = 1,\n                 pblstm_layers           = 1,\n                 listener_hidden_size    = 256,\n                 n_heads                 = 8,\n                 tf_blocks               = 1):\n        super().__init__()\n\n        # create an lstm layer\n        self.base_lstm      = pBLSTM(input_size = input_size,\n                                     num_layers = base_lstm_layers,\n                                     hidden_size = listener_hidden_size)# TODO\n\n        # create a sequence of Conv1d layers\n        self.embedding      = torch.nn.Sequential(\n              torch.nn.Conv1d(in_channels=listener_hidden_size*2,out_channels=listener_hidden_size,kernel_size=5,padding=2),\n              torch.nn.BatchNorm1d(listener_hidden_size),\n              torch.nn.GELU(),\n              torch.nn.Conv1d(in_channels=listener_hidden_size,out_channels= listener_hidden_size,kernel_size=5,padding=2)\n              )# TODO\n\n        # compute the postion encoding\n        self.positional_encoding    = PositionalEncoding(projection_size=listener_hidden_size)# TODO\n\n        # create a sequence of transformer blocks\n        self.transformer_encoder    = torch.nn.Sequential(\n            *[TransformerEncoder(projection_size=listener_hidden_size, num_heads =  n_heads) for _ in range(tf_blocks)]\n        )\n        # for i in range(tf_blocks):\n        #     # TODO\n\n    def forward(self, x, x_len):\n\n        # pack the inputs before passing them to the LSTm\n        x_packed                = pack_padded_sequence(x,x_len, batch_first=True, enforce_sorted = False) # TODO\n        # Pass the packed sequence through the lstm\n        lstm_out, _             = self.base_lstm(x_packed)# TODO\n        # Unpack the output of the lstm\n        output, output_lengths  = pad_packed_sequence(lstm_out, batch_first=True)# TODO\n\n        # Pass the output through the embedding\n        output                  = self.embedding(output.transpose(1,2)).transpose(1,2)# TODO\n        # calculate the new output length\n        output_lengths          = output_lengths//2# TODO\n\n        # calculate the position encoding\n        output  = self.positional_encoding(output)# TODO\n        # Pass the output of the positional encoding through the transformer encoder\n        output  = self.transformer_encoder(output)# TODO\n\n        return output, output_lengths","metadata":{"id":"F0opYqry_EGi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Attention\n\n### Different ways to compute Attention\n\n1. Dot-product attention\n    * raw_weights = bmm(key, query)\n    * Optional: Scaled dot-product by normalizing with sqrt key dimension\n    * Check \"Attention is All You Need\" Section 3.2.1\n    * 1st way is what most TAs are comfortable with, but if you want to explore, check out other methods below\n\n\n2. Cosine attention\n    * raw_weights = cosine(query, key) # almost the same as dot-product xD\n\n3. Bi-linear attention\n    * W = Linear transformation (learnable parameter): d_k -> d_q\n    * raw_weights = bmm(key @ W, query)\n\n4. Multi-layer perceptron\n    * Check \"Neural Machine Translation and Sequence-to-sequence Models: A Tutorial\" Section 8.4\n\n5. Multi-Head Attention\n    * Check \"Attention is All You Need\" Section 3.2.2\n    * h = Number of heads\n    * W_Q, W_K, W_V: Weight matrix for Q, K, V (h of them in total)\n    * W_O: d_v -> d_v\n    * Reshape K: (B, T, d_k) to (B, T, h, d_k // h) and transpose to (B, h, T, d_k // h)\n    * Reshape V: (B, T, d_v) to (B, T, h, d_v // h) and transpose to (B, h, T, d_v // h)\n    * Reshape Q: (B, d_q) to (B, h, d_q // h) `\n    * raw_weights = Q @ K^T\n    * masked_raw_weights = mask(raw_weights)\n    * attention = softmax(masked_raw_weights)\n    * multi_head = attention @ V\n    * multi_head = multi_head reshaped to (B, d_v)\n    * context = multi_head @ W_O","metadata":{"id":"5fG9jDZBVklL"}},{"cell_type":"markdown","source":"Pseudocode:\n\n```python\nclass Attention:\n    '''\n    Attention is calculated using the key, value (from encoder embeddings) and query from decoder.\n\n    After obtaining the raw weights, compute and return attention weights and context as follows.:\n\n    attention_weights   = softmax(raw_weights)\n    attention_context   = einsum(\"thinkwhatwouldbetheequationhere\",attention, value) #take hint from raw_weights calculation\n\n    At the end, you can pass context through a linear layer too.\n    '''\n\n    def init(listener_hidden_size,\n              speller_hidden_size,\n              projection_size):\n\n        VW = Linear(listener_hidden_size,projection_size)\n        KW = Linear(listener_hidden_size,projection_size)\n        QW = Linear(speller_hidden_size,projection_size)\n\n    def set_key_value(encoder_outputs):\n        '''\n        In this function we take the encoder embeddings and make key and values from it.\n        key.shape   = (batch_size, timesteps, projection_size)\n        value.shape = (batch_size, timesteps, projection_size)\n        '''\n        key = KW(encoder_outputs)\n        value = VW(encoder_outputs)\n      \n    def compute_context(decoder_context):\n        '''\n        In this function from decoder context, we make the query, and then we\n         multiply the queries with the keys to find the attention logits,\n         finally we take a softmax to calculate attention energy which gets\n         multiplied to the generted values and then gets summed.\n\n        key.shape   = (batch_size, timesteps, projection_size)\n        value.shape = (batch_size, timesteps, projection_size)\n        query.shape = (batch_size, projection_size)\n\n        You are also recomended to check out Abu's Lecture 19 to understand Attention better.\n        '''\n        query = QW(decoder_context) #(batch_size, projection_size)\n\n        raw_weights = #using bmm or einsum. We need to perform batch matrix multiplication. It is important you do this step correctly.\n        #What will be the shape of raw_weights?\n\n        attention_weights = #What makes raw_weights -> attention_weights\n\n        attention_context = #Multiply attention weights to values\n\n        return attention_context, attention_weights\n```","metadata":{"id":"wyv0Q65t5SDd"}},{"cell_type":"code","source":"class Attention(torch.nn.Module):\n  def __init__(self,\n        listener_hidden_size,\n        speller_hidden_size,\n        projection_size):\n    super().__init__()\n    self.KW = torch.nn.Linear(listener_hidden_size, projection_size)\n    self.VW = torch.nn.Linear(listener_hidden_size, projection_size)\n    self.QW = torch.nn.Linear(listener_hidden_size, projection_size)\n\n    self.softmax = nn.Softmax()\n\n\n  def set_key_value(self, encoder_outputs):\n\n    self.key = self.KW(encoder_outputs)\n    self.value = self.VW(encoder_outputs)\n\n    # print(\"key: \", self.key.shape)\n    # print(\"value: \", self.value.shape)\n\n  def compute_context(self, decoder_context):\n    self.query = self.QW(decoder_context)\n    self.query = torch.unsqueeze(self.query,2)\n    length_q = self.query.shape[1]\n    # print(\"key: \", self.key.shape)\n    # print(\"q: \", self.query.shape)\n    raw_weights = torch.bmm(self.key, self.query).squeeze()\n    attention_weights = self.softmax(raw_weights/np.sqrt(length_q))\n    attention_context = torch.einsum('bi,bij->bj', attention_weights,self.value)\n\n    return attention_context, attention_weights","metadata":{"id":"771TXxn7ViOW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The Speller\n\nSimilar to the language model that you coded up for HW4P1, you have to code a language model for HW4P2 as well. This time, we will also call the attention context step, within the decoder to get the attended-encoder-embeddings.\n\n\nWhat you have coded till now:\n\n<center>\n<img src=\"https://github.com/varunjain3/11785_s23_h4p2/raw/main/EncoderAttention.png\" alt=\"data flow\" height=\"400\">\n</center>\n\nFor the Speller, what we have to code:\n\n\n<center>\n<img src=\"https://github.com/varunjain3/11785_s23_h4p2/raw/main/Decoder.png\" alt=\"data flow\" height=\"400\">\n</center>","metadata":{"id":"4Sp1WywZmm1L"}},{"cell_type":"code","source":"class Speller(torch.nn.Module):\n\n  # Refer to your HW4P1 implementation for help with setting up the language model.\n  # The only thing you need to implement on top of your HW4P1 model is the attention module and teacher forcing.\n\n  def __init__(self,vocab_size, embedding_size, projection_size, speller_size,  attender:Attention):\n    super(). __init__()\n\n    self.projection_size = projection_size\n    self.attend = attender # Attention object in speller\n    self.max_timesteps = 590# Max timesteps\n\n    lstm_size = embedding_size+projection_size\n    self.embedding =  torch.nn.Embedding(vocab_size, embedding_size) # Embedding layer to convert token to latent space\n    self.lstm_cells =  torch.nn.Sequential(\n            torch.nn.LSTMCell(lstm_size, speller_size),\n            torch.nn.LSTMCell(speller_size, speller_size),\n            torch.nn.LSTMCell(speller_size, speller_size)\n        )# Create a sequence of LSTM Cells\n\n    # For CDN (Feel free to change)\n    self.output_to_char = torch.nn.Linear(speller_size+projection_size, embedding_size)# Linear module to convert outputs to correct hidden size (Optional: TO make dimensions match)\n    self.activation = torch.nn.GELU()# Check which activation is suggested\n    self.char_prob = torch.nn.Linear(embedding_size, vocab_size)# Linear layer to convert hidden space back to logits for token classification\n    self.char_prob.weight = self.embedding.weight# Weight tying (From embedding layer)\n    self.drop = torch.nn.Dropout(p=0.21)\n\n  def lstm_step(self, input_word, hidden_state):\n\n    embedding =  input_word\n\n    for i in range(len(self.lstm_cells)):\n        embedding, cell_state = self.lstm_cells[i](embedding,hidden_state[i]) # Feed the input through each LSTM Cell\n\n\n        embedding = self.drop(embedding)\n        hidden_state[i] = (embedding, cell_state)\n\n    return embedding, hidden_state # What information does forward() need?\n\n\n  def CDN(self,input):\n    # Make the CDN here, you can add the output-to-char\n    # raise NotImplementedError\n    out = self.activation(self.output_to_char(self.drop(input)))\n    # out = self.activation(self.linear(self.drop(out)))\n    out = self.char_prob(out)\n    return out\n\n  def forward (self, y=None, teacher_forcing_ratio=1, encoder_batch_size=x.shape[0]):\n\n    batch_size = encoder_batch_size #y.shape[0] #x.shape[0]\n    attn_context = torch.zeros(batch_size, self.projection_size).to(DEVICE)# initial context tensor for time t = 0\n    output_symbol = torch.full((batch_size,), SOS_TOKEN).to(DEVICE)# Set it to SOS for time t = 0\n    raw_outputs = []\n    attention_plot = []\n\n    if y is None:\n      timesteps = self.max_timesteps\n      teacher_forcing_ratio = 0 #Why does it become zero?\n\n    else:\n      timesteps = y.shape[1] # How many timesteps are we predicting for?\n\n    hidden_states_list = [None] * len(self.lstm_cells)# Initialize your hidden_states list here similar to HW4P1\n\n    for t in range(timesteps):\n      p = np.random.uniform(0,1)# generate a probability p between 0 and 1\n\n      if p < teacher_forcing_ratio and t > 0: # Why do we consider cases only when t > 0? What is considered when t == 0? Think.\n        output_symbol = y[:,t-1] # Take from y, else draw from probability distribution\n\n\n      char_embed = self.embedding(output_symbol) # Embed the character symbol\n\n      # Concatenate the character embedding and context from attention, as shown in the diagram\n      lstm_input = torch.cat((char_embed,attn_context), dim = 1)\n\n      # print(\"char_embed\",char_embed.shape)\n      # print(\"attn_context\",attn_context.shape)\n      # print(\"lstm_input\",lstm_input.shape)\n\n      lstm_out, hidden_states_list = self.lstm_step(lstm_input, hidden_states_list) # Feed the input through LSTM Cells and attention.\n      # What should we retrieve from forward_step to prepare for the next timestep?\n\n      # print(\"lstm_out\",lstm_out.shape)\n\n      attn_context, attn_weights = self.attend.compute_context(lstm_out) # Feed the resulting hidden state into attention\n\n      cdn_input = torch.cat((lstm_out,attn_context), dim = 1)# TODO: You need to concatenate the context from the attention module with the LSTM output hidden state, as shown in the diagram\n\n      raw_pred = self.CDN(cdn_input) # call CDN with cdn_input\n\n      # Generate a prediction for this timestep and collect it in output_symbols\n      output_symbol =  torch.argmax(raw_pred, dim = 1)# Draw correctly from raw_pred\n\n      raw_outputs.append(raw_pred) # for loss calculation\n      attention_plot.append(attn_weights) # for plotting attention plot\n\n\n    attention_plot = torch.stack(attention_plot, dim=1)\n    raw_outputs = torch.stack(raw_outputs, dim=1)\n\n    return raw_outputs, attention_plot","metadata":{"id":"nFkc6MbnlUPu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ASRModel(torch.nn.Module):\n  def __init__(self, vocab_size, embedding_size, input_size, encoder_hidden_size, listener_size, speller_size, projection_size, name=\"ASR\"): # add parameters\n    super().__init__()\n\n    # Pass the right parameters here\n    self.listener = TransformerListener(input_size, listener_hidden_size=encoder_hidden_size)\n    self.attend = Attention(listener_size, speller_size, projection_size)\n    self.speller = Speller(vocab_size, embedding_size, projection_size, speller_size, self.attend)\n    self.name = name\n\n  def forward(self, x,lx,y=None,teacher_forcing_ratio=1):\n    # Encode speech features\n    encoder_outputs, _ = self.listener(x,lx)\n\n    # We want to compute keys and values ahead of the decoding step, as they are constant for all timesteps\n    # Set keys and values using the encoder outputs\n    self.attend.set_key_value(encoder_outputs)\n\n    # Decode text with the speller using context from the attention\n    raw_outputs, attention_plots = self.speller(y=y,teacher_forcing_ratio=teacher_forcing_ratio,\n                                                encoder_batch_size = encoder_outputs.shape[0])\n\n    return raw_outputs, attention_plots","metadata":{"id":"scvB2cI-OSof"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Setup","metadata":{"id":"bPZD3vqdUisj"}},{"cell_type":"code","source":"torch.cuda.empty_cache()\ngc.collect()","metadata":{"id":"p2uX2P9YVnbk","outputId":"fa778278-8a37-4c95-8c6e-72e94a0ece96"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = ASRModel(\n\n    # Initialize your model\n    vocab_size=31,\n    embedding_size=580,\n    input_size=28,\n    encoder_hidden_size=256,\n    listener_size = 256,\n    speller_size = 256,\n    projection_size = 256\n)\n\nmodel = model.to(DEVICE)\nprint(model)","metadata":{"id":"a9LN0l5VUk_s","outputId":"e5e6003d-1899-48fd-fc1a-7584da3afd4c","scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loss Function, Optimizers, Scheduler","metadata":{"id":"23DMfXsaU6kj"}},{"cell_type":"code","source":"optimizer   = torch.optim.AdamW(model.parameters(), lr= config['learning_rate'])\n\ncriterion   = torch.nn.CrossEntropyLoss(reduction='mean',ignore_index=PAD_TOKEN)\n\nscaler      = torch.cuda.amp.GradScaler()\n\nscheduler   = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = 70, eta_min = 0.000001)\n","metadata":{"id":"216ukmHbU-ol"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Levenshtein Distance","metadata":{"id":"ZWQnB8lUVY4f"}},{"cell_type":"code","source":"# We have given you this utility function which takes a sequence of indices and converts them to a list of characters\ndef indices_to_chars(indices, vocab):\n    tokens = []\n    for i in indices: # This loops through all the indices\n        if int(i) == SOS_TOKEN: # If SOS is encountered, dont add it to the final list\n            continue\n        elif int(i) == EOS_TOKEN: # If EOS is encountered, stop the decoding process\n            break\n        else:\n            tokens.append(vocab[i])\n    return tokens\n\n# To make your life more easier, we have given the Levenshtein distantce / Edit distance calculation code\ndef calc_edit_distance(predictions, y, y_len, vocab= VOCAB, print_example= False):\n\n    dist                = 0\n    batch_size, seq_len = predictions.shape\n\n    for batch_idx in range(batch_size):\n\n        y_sliced    = indices_to_chars(y[batch_idx,0:y_len[batch_idx]], vocab)\n        pred_sliced = indices_to_chars(predictions[batch_idx], vocab)\n\n        # Strings - When you are using characters from the AudioDataset\n        y_string    = ''.join(y_sliced)\n        pred_string = ''.join(pred_sliced)\n\n        dist        += Levenshtein.distance(pred_string, y_string)\n        # Comment the above abd uncomment below for toy dataset\n        # dist      += Levenshtein.distance(y_sliced, pred_sliced)\n\n    if print_example:\n        # Print y_sliced and pred_sliced if you are using the toy dataset\n        print(\"\\nGround Truth : \", y_string)\n        print(\"Prediction   : \", pred_string)\n\n    dist    /= batch_size\n    return dist","metadata":{"id":"rSsiCdxPVeZW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train and Validation functions\n","metadata":{"id":"Pu4MrSMUUIyp"}},{"cell_type":"code","source":"def train(model, dataloader, criterion, optimizer, teacher_forcing_rate):\n\n    model.train()\n    batch_bar = blue_tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n\n    running_loss        = 0.0\n    running_perplexity  = 0.0\n\n    for i, (x, y, lx, ly) in enumerate(dataloader):\n        # if i < 1083: #for debugging purposes\n        #   continue\n\n        optimizer.zero_grad()\n\n        x, y, lx, ly = x.to(DEVICE), y.to(DEVICE), lx, ly\n\n        with torch.cuda.amp.autocast():\n\n            raw_predictions, attention_plot = model(x, lx, y = y, teacher_forcing_ratio= teacher_forcing_rate)\n\n            # Predictions are of Shape (batch_size, timesteps, vocab_size).\n            # Transcripts are of shape (batch_size, timesteps) Which means that you have batch_size amount of batches with timestep number of tokens.\n            # So in total, you have batch_size*timesteps amount of characters.\n            # Similarly, in predictions, you have batch_size*timesteps amount of probability distributions.\n            # How do you need to modify transcipts and predictions so that you can calculate the CrossEntropyLoss? Hint: Use Reshape/View and read the docs\n            # Also we recommend you plot the attention weights, you should get convergence in around 10 epochs, if not, there could be something wrong with\n            # your implementation\n\n\n            raw_predictions = torch.permute(raw_predictions, (0,2,1))\n            loss        =  criterion(raw_predictions, y)# TODO: Cross Entropy Loss\n\n            perplexity  = torch.exp(loss) # Perplexity is defined the exponential of the loss\n\n            running_loss        += loss.item()\n            running_perplexity  += perplexity.item()\n\n        # Backward on the masked loss\n        scaler.scale(loss).backward()\n\n        # Optional: Use torch.nn.utils.clip_grad_norm to clip gradients to prevent them from exploding, if necessary\n        # If using with mixed precision, unscale the Optimizer First before doing gradient clipping\n\n        scaler.step(optimizer)\n        scaler.update()\n\n\n        batch_bar.set_postfix(\n            loss=\"{:.04f}\".format(running_loss/(i+1)),\n            perplexity=\"{:.04f}\".format(running_perplexity/(i+1)),\n            lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])),\n            tf_rate='{:.02f}'.format(teacher_forcing_rate))\n        batch_bar.update()\n\n        del x, y, lx, ly\n        torch.cuda.empty_cache()\n\n    running_loss /= len(dataloader)\n    running_perplexity /= len(dataloader)\n    batch_bar.close()\n\n    return running_loss, running_perplexity, attention_plot","metadata":{"id":"gYRVKs9_2rsx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def validate(model, dataloader):\n\n    model.eval()\n\n    batch_bar = blue_tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc=\"Val\")\n\n    running_lev_dist = 0.0\n    my_count = 0\n\n    for i, (x, y, lx, ly) in enumerate(dataloader):\n\n        if i%500 != 0: #for debugging purposes\n          continue\n        my_count += 1\n        x, y, lx, ly = x.to(DEVICE), y.to(DEVICE), lx, ly\n\n        with torch.inference_mode():\n            raw_predictions, attentions = model(x, lx, y = None)\n\n        # Greedy Decoding\n        print(i,\"a\",end=\"\\r\")\n        greedy_predictions   =  torch.argmax(raw_predictions, dim=2)# TODO: How do you get the most likely character from each distribution in the batch?\n\n        print(i,\"a**********\",end=\"\\r\")\n        # Calculate Levenshtein Distance\n        running_lev_dist    += calc_edit_distance(greedy_predictions, y, ly, VOCAB, print_example = False) # You can use print_example = True for one specific index i in your batches if you want\n\n        batch_bar.set_postfix(\n            dist=\"{:.04f}\".format(running_lev_dist/(i+1)))\n        batch_bar.update()\n\n        del x, y, lx, ly\n        torch.cuda.empty_cache()\n\n    batch_bar.close()\n    running_lev_dist /= my_count #len(dataloader) # FOR DEBUGGING PURPOSES\n\n    return running_lev_dist","metadata":{"id":"uIx3tW7a2tze"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Wandb\n","metadata":{"id":"WhwhevgWQbDX"}},{"cell_type":"code","source":"# Login to Wandb\n# Initialize your Wandb Run Here\n# Save your model architecture in a txt file, and save the file to Wandb","metadata":{"id":"1Xbw_0eAQcoR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Experiment","metadata":{"id":"JmZhxhNseaIr"}},{"cell_type":"code","source":"best_lev_dist = float(\"inf\")\ntf_rate = 1.0\n\nfor epoch in range(0, config['epochs']):\n\n    print(\"\\nEpoch: {}/{}\".format(epoch+1, config['epochs']))\n\n    curr_lr = float(optimizer.param_groups[0]['lr'])\n    # Call train and validate, get attention weights from training\n\n    train_loss, running_perplexity, attention_plot = train(model,train_loader, criterion, optimizer,tf_rate)\n    valid_dist = validate(model, valid_loader)\n\n\n    # Print your metrics\n    print(\"\\tTrain Loss {:.04f}\\t Learning Rate {:.07f}\".format(train_loss, curr_lr))\n    print(\"\\tVal Dist {:.04f}\\t\".format(valid_dist))\n    print(\"\\tTeacher Forcing Ratio {}\\t\".format(tf_rate))\n\n\n    # Plot Attention for a single item in the batch\n    plot_attention(attention_plot[0].cpu().detach().numpy())\n\n    # Log metrics to Wandb\n\n    # wandb.log({\n    #     'train_loss': train_loss,\n    #     'valid_dist': valid_dist,\n    #     'lr'        : curr_lr,\n    #     'tf_ratio'  : tf_rate\n    # })\n    # Optional: Scheduler Step / Teacher Force Schedule Step\n\n    scheduler.step()\n\n    if valid_dist <= best_lev_dist:\n        best_lev_dist = valid_dist\n        # Save your model checkpoint here\n        save_path = \"Run3_model_{0}_{1}_{2}.pth\".format(config['learning_rate'],config['batch_size'],model.name)\n        save_model(model, optimizer, scheduler, \"tf_scheduler\",  ['valid_dist', valid_dist], epoch, save_path)\n        # (model, optimizer, scheduler, tf_scheduler, metric, epoch, path)\n        # wandb.save(save_path)\n        print(\"Saved best model\")","metadata":{"id":"JcTFu-AH3m4e","outputId":"f865ded1-0388-49e5-c544-3bd094842f3a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing","metadata":{"id":"hgFYFaBGeBqM"}},{"cell_type":"code","source":"# Optional: Load your best model Checkpoint here\n\n# TODO: Create a testing function similar to validation\n# TODO: Create a file with all predictions\n# TODO: Submit to Kaggle","metadata":{"id":"ndNCpcxkx2KG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def testing(model, dataloader):\n\n    results = []\n    model.eval()\n\n    # batch_bar = blue_tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc=\"Testing\")\n\n    running_lev_dist = 0.0\n\n    for i, (x,lx) in enumerate(dataloader):\n\n        x, lx = x.to(DEVICE), lx\n\n        with torch.inference_mode():\n            raw_predictions, attentions = model(x, lx, y = None)\n\n        greedy_predictions   = torch.argmax(raw_predictions, dim=2) # TODO: How do you get the most likely character from each distribution in the batch?\n\n        del x, lx\n        results.extend(greedy_predictions)\n    return results","metadata":{"id":"3sCc4TH6u6RJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint = torch.load(\"/content/Run3_model_0.001_150_LAS.pth\")\nmodel_dict = checkpoint[\"model_state_dict\"]\n# opt_dict = checkpoint[\"optimizer_state_dict\"]\n# sched_dict = checkpoint[\"scheduler_state_dict\"]\nmodel.load_state_dict(model_dict)\n# optimizer.load_state_dict(opt_dict)\n# scheduler.load_state_dict(sched_dict)\n","metadata":{"id":"pxhrfEe-u6RK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = testing(model, test_loader)","metadata":{"id":"leB7xhCmu6RK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# results","metadata":{"id":"-YkarOcSRyxE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('submission.csv', 'w') as file:\n  file.write(\"index,label\\n\")\n  for i, pred in enumerate(results):\n    pred_sliced = indices_to_chars(results[i], VOCAB)\n    pred_string = ''.join(pred_sliced)\n    file.write(f\"{i},{pred_string}\\n\")","metadata":{"id":"iYCk7DsWTbIs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!kaggle competitions submit -c attention-based-speech-recognition -f submission.csv -m \"initial sub\"","metadata":{"id":"_dSJNYDRu6RK","outputId":"52f9c7b7-1e97-4714-f3fa-a279c6a4c4e4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"AI-JfcyPUgWl"},"execution_count":null,"outputs":[]}]}
